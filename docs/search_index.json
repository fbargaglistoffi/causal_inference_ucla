[["index.html", "Welcome Course Description Instructor Teaching Assistants Course Logistics Required Materials Learning Objectives Upon successful completion of this course, students will be able to: Tools Useful Links Acknowledgments", " Welcome Welcome to the course website for The Science of Why: Causal Inference for Public Health. This course introduces the foundations of causal inference in public health and medical sciences, with an emphasis on distinguishing between association and causation in real-world research. Course Description In this course, students will: Explore foundational concepts such as counterfactuals, causal estimands, and identification strategies. Learn how to critically evaluate causal claims in the medical and public health literature. Understand and apply experimental and observational study designs, causal diagrams, and bias adjustment methods. Develop practical skills through R-based tutorials and case studies. This course is ideal for students interested in public health, epidemiology, biostatistics, or social science research. Instructor Falco J. Bargagli-Stoffi Assistant Professor Department of Biostatistics UCLA Fielding School of Public Health falco@ucla.edu Office hours: Right after class (1 hour) Teaching Assistants TBD — will be announced on the course website. Course Logistics Offered: Fall Term Meetings: Lecture (3 hours), Discussion (1 hour) — in-person unless otherwise announced Assignments: Posted on the course website, due before 8 PM on specified dates Final Project: Group presentation + written report during finals week Required Materials Primary Textbook: Hernán MA, Robins JM. (2023) Causal Inference: What If Free PDF available here Secondary (Recommended) Texts: - Rosenbaum PR. Causal Inference. MIT Press. - Pearl J., Mackenzie D. The Book of Why. Basic Books. Supplementary Readings: Will be shared throughout the course. Learning Objectives This course introduces students to the foundations of causal inference in public health and medical sciences, with a strong emphasis on the difference between association and causation. Students will learn to critically evaluate causal claims, understand causal diagrams (DAGs), and assess study design choices in both randomized and observational studies. We will explore how causal effects are identified and estimated, how to detect and adjust for biases (like confounding and selection bias), and how to use critical thinking tools when reviewing public health research. Students will also be introduced to concepts like effect modification, interaction, and systems thinking through real-world applications. Upon successful completion of this course, students will be able to: Communicate medical and public health findings and causal claims in written and oral forms Evaluate causal claims in academic and public health literature Design and assess both randomized trials and observational studies Construct and analyze causal diagrams for identifying sources of bias Apply concepts of effect modification, interaction, and confounding Interpret findings within the context of public health policy and practice Work independently and collaboratively to assess causal research Tools This course will make use of: - R and RStudio - Interactive R tutorials and guided analysis - GitHub for accessing materials and submitting assignments Useful Links UCLA Center for Accessible Education UCLA Equity, Diversity, and Inclusion FSPH EDI Initiative Acknowledgments Special thanks to Charlie Wang, Shravani Chiddarwar and Kevin Ngo for their invaluable help in preparing this course. "],["setup.html", "Setup Installing and Using Required Packages Loading Packages Difference Between install.packages() and library() Session Information Why include this? Base R version (simple)", " Setup Installing and Using Required Packages Throughout this tutorial, we’ll use a few essential R packages to manipulate data, run models, and create plots. Below are the core packages, what they do, and how to install them. Packages we’ll use: ggplot2 – For plotting (e.g., scatterplots, regression lines) dplyr – For data wrangling (filtering, mutating, summarizing, etc.) gridExtra – To combine multiple plots into one figure stats – Comes with base R and used for regression (lm()) pacman – Simplifies package management in R broom (optional) – Makes model summaries easier to work with # install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;gridExtra&quot;, &quot;pacman&quot;, &quot;broom&quot;)) Loading Packages library(ggplot2) library(dplyr) library(gridExtra) # Optional if using tidy model outputs: # library(broom) Difference Between install.packages() and library() install.packages(“dplyr”) downloads and installs the package — you only need to do this once per computer. library(dplyr) loads the package into your R session — you need to run this each time you use it. Session Information It’s always good practice to include your session info at the end of your analysis. This gives a snapshot of: Your R version and system details All the packages that were loaded The versions of those packages This is especially useful when: - You’re debugging errors - You’re submitting assignments - You’re collaborating with others Why include this? Sometimes code behaves differently depending on the version of a package or even the version of R itself. Including your session info makes your work reproducible and easier to troubleshoot. Base R version (simple) This function comes with R and gives you basic session details. sessionInfo() ## R version 4.5.1 (2025-06-13) ## Platform: aarch64-apple-darwin20 ## Running under: macOS Sequoia 15.7.3 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib; LAPACK version 3.12.1 ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## time zone: America/Los_Angeles ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] gridExtra_2.3 dplyr_1.1.4 ggplot2_3.5.2 igraph_2.2.1 bnlearn_5.1 cobalt_4.6.0 bookdown_0.46 ## ## loaded via a namespace (and not attached): ## [1] gtable_0.3.6 jsonlite_2.0.0 compiler_4.5.1 crayon_1.5.3 tidyselect_1.2.1 parallel_4.5.1 ## [7] jquerylib_0.1.4 scales_1.4.0 yaml_2.3.10 fastmap_1.2.0 R6_2.6.1 labeling_0.4.3 ## [13] generics_0.1.4 knitr_1.50 tibble_3.3.0 chk_0.10.0 bslib_0.9.0 pillar_1.11.0 ## [19] RColorBrewer_1.1-3 rlang_1.1.6 cachem_1.1.0 xfun_0.52 sass_0.4.10 cli_3.6.5 ## [25] withr_3.0.2 magrittr_2.0.3 digest_0.6.37 grid_4.5.1 rstudioapi_0.17.1 lifecycle_1.0.4 ## [31] vctrs_0.6.5 evaluate_1.0.4 glue_1.8.0 farver_2.1.2 rmarkdown_2.29 tools_4.5.1 ## [37] pkgconfig_2.0.3 htmltools_0.5.8.1 "],["causal-dictionary.html", "Causal Dictionary", " Causal Dictionary We compiled a list of important terms or common questions you might have! This page is useful to refer to throughout the course. Conditioning - A statistical operation to analyze relationships of variables while holding values of other variables constant. For example, if we are analyzing the relationship between \\(X\\) and \\(Y\\) while conditioning on \\(Z\\), we are asking “What is the relationship of \\(X\\) and \\(Y\\) when holding \\(Z\\) constant?” Controlling/Adjusting - Controlling or adjusting for a variable is one way to implement conditioning, particularly in regression contexts. We control for a variable by adding it as a covariate to our regression model. The regression coefficient for \\(X\\) then represents the association between \\(X\\) and \\(Y\\) when \\(Z\\) is held constant. Stratifying/Selecting - Stratifying or selecting is a different way to implement conditioning by subsetting the data to observations that meets certain criteria. We then only analyze that subset of the data. For example, if we are interested in the effect of an energy drink on a person’s awareness levels and we only include college students in the study, then we are stratifying. What is the difference between controlling and stratifying for Z? - For example let \\(Z\\) = sex assigned at birth. Both controlling and stratifying are forms of conditioning. When we control, we add \\(Z\\) as a covariate to our regression model, but our data still contains both males and females. When we stratify for males, then only males are included in our analysis. Though they are both forms of conditioning, controlling or stratifying have different implications for what causal quantity we are actually estimating. Selection bias - Selection bias occurs when the probability of being included in the analysis depends on a factor that affects the association between the treatment and outcome. "],["foundations-of-causal-thinking.html", "1 Foundations of Causal Thinking Class materials Textbook Reading Supplementary Reading Topics Covered 1.1 Association vs. Causation 1.2 Why We Use Simulations 1.3 Introduction to Counterfactuals and Potential Outcomes 1.4 Causal Estimands and Identification 1.5 Measuring Effects for Binary Outcomes", " 1 Foundations of Causal Thinking Class materials Slides: Module 1 Textbook Reading Hernán &amp; Robins, Causal Inference: What If – Chapter 1 Supplementary Reading Pearl, J. and Mackenzie, D. (2018) The Book of Why: The New Science of Cause and Effect. Basic Books. Selected public health news articles (provided on the course site). Topics Covered Association vs. Causation Introduction to Counterfactuals and Potential Outcomes Causal Estimands and Identification Measuring Effects for Binary Outcomes Critical reading exercise: analyzing causal claims in public health news 1.1 Association vs. Causation Association refers to a statistical relationship where two variables move together, but one doesn’t necessarily cause the other. For instance, ice cream sales and drowning incidents both rise in the summer, not because one causes the other, but because they share a third factor: temperature. In contrast, causation implies a direct cause-and-effect relationship, where changing one variable leads to changes in another. Establishing causation requires rigorous methods, such as randomized controlled trials, to rule out confounding factors. Simpson’s Paradox occurs when a trend appears in separate groups but reverses when the data are combined. This paradox is driven by confounding variables, unaccounted factors that influence both the treatment and the outcome. It illustrates how aggregated data can be misleading and emphasizes the importance of analyzing relationships within subgroups to avoid drawing incorrect conclusions. To demonstrate this paradox, we simulate a study comparing two pneumonia treatments across 2,000 people. Treatment A was mostly given to mild cases, while Treatment B was given to severe cases. When data are analyzed without considering severity, Treatment A seems more effective. However, when stratified by severity, Treatment B consistently shows lower death rates in both mild and severe groups. Checklist Item 1: What is the causal question? Before looking at the data, let’s define the question: What is the effect of Treatment B (vs. Treatment A) on mortality risk in pneumonia patients? Treatment (\\(W\\)): Treatment B vs. Treatment A. Outcome (\\(Y\\)): Death (binary). Population: Pneumonia patients. Estimand: Risk difference. 1.1.1 Simulating Simpson’s Paradox set.seed(123) n &lt;- 2050 severity &lt;- rep(c(&quot;Mild&quot;, &quot;Severe&quot;), times = c(1450, 600)) treatment &lt;- c(rep(&quot;Treatment A&quot;, 1400), rep(&quot;Treatment B&quot;, 50), rep(&quot;Treatment A&quot;, 100), rep(&quot;Treatment B&quot;, 500)) outcome &lt;- c(rbinom(1400, 1, 0.15), # Mild + A (15% death rate) rbinom(50, 1, 0.10), # Mild + B (10% death rate) rbinom(100, 1, 0.30), # Severe + A (30% death rate) rbinom(500, 1, 0.20)) # Severe + B (20% death rate) df &lt;- data.frame( Severity = severity, Treatment = treatment, Outcome = outcome ) # Compute death rates by severity and treatment death_counts &lt;- tapply(df$Outcome, list(df$Severity, df$Treatment), sum) table_counts &lt;- table(df$Severity, df$Treatment) death_rates &lt;- round(death_counts / table_counts, 3) overall_a &lt;- sum(df$Outcome[df$Treatment == &quot;Treatment A&quot;]) / sum(df$Treatment == &quot;Treatment A&quot;) overall_b &lt;- sum(df$Outcome[df$Treatment == &quot;Treatment B&quot;]) / sum(df$Treatment == &quot;Treatment B&quot;) print(&quot;Death rates by severity and treatment:&quot;) ## [1] &quot;Death rates by severity and treatment:&quot; print(death_rates) ## Treatment A Treatment B ## Mild 0.136 0.120 ## Severe 0.380 0.204 cat(&quot;Overall death rate (Treatment A):&quot;, round(overall_a, 3), &quot;\\n&quot;) ## Overall death rate (Treatment A): 0.153 cat(&quot;Overall death rate (Treatment B):&quot;, round(overall_b, 3), &quot;\\n&quot;) ## Overall death rate (Treatment B): 0.196 1.1.2 Visualizing Simpson’s Paradox The plots below mirror the side-by-side comparison from the slides: the left panel shows the aggregated (misleading) view, while the right panel stratifies by severity to reveal the truth. # Compute stratified death rates group_rates &lt;- df %&gt;% group_by(Severity, Treatment) %&gt;% summarize(Death_rate = mean(Outcome), .groups = &quot;drop&quot;) # Build the two panels as separate data frames overall_df &lt;- data.frame( Panel = &quot;Aggregated (Misleading)&quot;, Group = c(&quot;Treatment A&quot;, &quot;Treatment B&quot;), Treatment = c(&quot;Treatment A&quot;, &quot;Treatment B&quot;), Death_rate = c(overall_a, overall_b) ) stratified_df &lt;- group_rates %&gt;% mutate( Panel = &quot;Stratified by Severity (Truth)&quot;, Group = paste(Severity, &quot;-&quot;, Treatment) ) # Combine plot_df &lt;- bind_rows( overall_df %&gt;% select(Panel, Group, Treatment, Death_rate), stratified_df %&gt;% select(Panel, Group, Treatment, Death_rate) ) # Set factor order so panels and bars appear in the right sequence plot_df$Panel &lt;- factor(plot_df$Panel, levels = c(&quot;Aggregated (Misleading)&quot;, &quot;Stratified by Severity (Truth)&quot;)) plot_df$Group &lt;- factor(plot_df$Group, levels = c(&quot;Treatment A&quot;, &quot;Treatment B&quot;, &quot;Mild - Treatment A&quot;, &quot;Mild - Treatment B&quot;, &quot;Severe - Treatment A&quot;, &quot;Severe - Treatment B&quot;)) ggplot(plot_df, aes(x = Group, y = Death_rate, fill = Treatment)) + geom_col(width = 0.65, alpha = 0.85) + geom_text(aes(label = paste0(round(Death_rate * 100, 1), &quot;%&quot;)), vjust = -0.5, size = 3.5) + facet_wrap(~ Panel, scales = &quot;free_x&quot;) + scale_fill_manual(values = c(&quot;Treatment A&quot; = &quot;#1F77B4&quot;, &quot;Treatment B&quot; = &quot;#D62728&quot;)) + scale_y_continuous(labels = scales::percent_format(), limits = c(0, max(plot_df$Death_rate) * 1.15)) + labs(title = &quot;Simpson&#39;s Paradox: Aggregated vs. Stratified Results&quot;, y = &quot;Death Rate&quot;, x = &quot;&quot;) + theme_minimal() + theme(axis.text.x = element_text(size = 8, angle = 20, hjust = 1), strip.text = element_text(face = &quot;bold&quot;, size = 11)) Only after stratifying by the severity of the case are we able to observe that Treatment B is actually more effective than Treatment A for both mild and severe pneumonia cases. The confounding variable (severity) was distorting the overall comparison. 1.2 Why We Use Simulations Throughout this course, we use simulations. Simulations are computer-generated datasets where we control the true data-generating process. In the real world, we never know the true causal effect. In a simulation, we do. This lets us: See both potential outcomes \\(Y(0)\\) and \\(Y(1)\\) for every person (impossible in reality) Compute the true causal effect and compare it to what an analyst would estimate from observed data Understand exactly when and why naive methods go wrong Think of a simulation as a laboratory for causal reasoning: we build a miniature world, set the rules, and then test whether our statistical tools recover the truth. The code below builds that intuition step by step. 1.3 Introduction to Counterfactuals and Potential Outcomes At the heart of causal inference lies a simple yet powerful idea: counterfactuals, meaning what would have happened if something else had occurred. However, we can never observe both outcomes for the same person. This is known as the Fundamental Problem of Causal Inference. We only observe the outcome under the condition that actually occurred, and everything else is unobserved, or counterfactual. Building on the concept of counterfactuals, the Average Treatment Effect (ATE) provides a formal way to quantify the impact of a treatment or intervention across a population. Since we cannot observe both potential outcomes (treated and untreated) for the same individual, ATE instead compares the average outcome we would see if everyone received the treatment versus if no one did. Mathematically, it is the difference between the expected value of the potential outcome under treatment and the expected value under control. While individual causal effects remain unobservable, the ATE offers a population-level summary of the treatment’s impact, a cornerstone of policy evaluation, randomized experiments, and observational causal analysis. 1.3.1 Notation Counterfactuals can be represented using potential outcomes notation: \\(Y\\) = the observed outcome \\(Y(0)\\) = the potential outcome under no treatment \\(Y(1)\\) = the potential outcome under treatment \\(W\\) = a binary variable that represents whether a unit was treated or not. If \\(W = 1\\), then the unit was treated. If \\(W = 0\\), then the unit was not treated. Notice that the observed outcome can be expressed in terms of potential outcomes: \\[Y = W \\cdot Y(1) + (1 - W) \\cdot Y(0)\\] So \\(Y = Y(1)\\) if the unit was treated and \\(Y = Y(0)\\) if the unit was not treated. This equation is known as consistency. 1.3.2 The Potential Outcomes Table Let’s make this concrete. We simulate 6 patients and, because this is a simulation, we get to see both potential outcomes for each person. This mirrors the HRT table from the slides. set.seed(42) # Six patients with known potential outcomes (headache intensity, 0-100) po_table &lt;- data.frame( Patient = 1:6, Y0 = c(75, 60, 80, 50, 70, 85), # outcome without aspirin Y1 = c(25, 40, 55, 30, 50, 60) # outcome with aspirin ) po_table$Causal_Effect &lt;- po_table$Y1 - po_table$Y0 cat(&quot;=== God&#39;s-Eye View: Both Potential Outcomes ===\\n\\n&quot;) ## === God&#39;s-Eye View: Both Potential Outcomes === knitr::kable(po_table, col.names = c(&quot;Patient&quot;, &quot;Y(0)&quot;, &quot;Y(1)&quot;, &quot;Causal Effect&quot;), caption = &quot;Complete potential outcomes table (only possible in a simulation)&quot;) Table 1.1: Complete potential outcomes table (only possible in a simulation) Patient Y(0) Y(1) Causal Effect 1 75 25 -50 2 60 40 -20 3 80 55 -25 4 50 30 -20 5 70 50 -20 6 85 60 -25 cat(&quot;\\nTrue ATE:&quot;, mean(po_table$Causal_Effect), &quot;\\n&quot;) ## ## True ATE: -26.66667 Every patient benefits from aspirin (all causal effects are negative). The ATE tells us the average benefit. But in the real world, we cannot see both columns. Each patient either takes aspirin or doesn’t. 1.3.3 What Happens When Treatment Is Assigned? Now let’s assign treatment and see how the table changes. We mask the counterfactual outcome with “?”. This is the Fundamental Problem of Causal Inference in action. # Assign treatment (not randomly; sicker patients more likely to get aspirin) po_table$W &lt;- c(1, 0, 1, 0, 1, 0) # patients 1, 3, 5 get aspirin # What we actually observe po_table$Y_obs &lt;- ifelse(po_table$W == 1, po_table$Y1, po_table$Y0) # Build the &quot;observed&quot; table with ?&#39;s obs_display &lt;- data.frame( Patient = po_table$Patient, W = ifelse(po_table$W == 1, &quot;Aspirin&quot;, &quot;No aspirin&quot;), Y0_obs = ifelse(po_table$W == 0, po_table$Y0, &quot;?&quot;), Y1_obs = ifelse(po_table$W == 1, po_table$Y1, &quot;?&quot;), Y_observed = po_table$Y_obs ) cat(&quot;=== What We Actually See ===\\n\\n&quot;) ## === What We Actually See === knitr::kable(obs_display, col.names = c(&quot;Patient&quot;, &quot;Treatment&quot;, &quot;Y(0)&quot;, &quot;Y(1)&quot;, &quot;Observed Y&quot;), caption = &quot;After treatment assignment: counterfactuals become &#39;?&#39;&quot;) Table 1.2: After treatment assignment: counterfactuals become ‘?’ Patient Treatment Y(0) Y(1) Observed Y 1 Aspirin ? 25 25 2 No aspirin 60 ? 60 3 Aspirin ? 55 55 4 No aspirin 50 ? 50 5 Aspirin ? 50 50 6 No aspirin 85 ? 85 # Naive comparison mean_treated &lt;- mean(po_table$Y_obs[po_table$W == 1]) mean_control &lt;- mean(po_table$Y_obs[po_table$W == 0]) cat(&quot;\\nMean outcome (treated):&quot;, mean_treated, &quot;\\n&quot;) ## ## Mean outcome (treated): 43.33333 cat(&quot;Mean outcome (control):&quot;, mean_control, &quot;\\n&quot;) ## Mean outcome (control): 65 cat(&quot;Naive estimate (treated - control):&quot;, mean_treated - mean_control, &quot;\\n&quot;) ## Naive estimate (treated - control): -21.66667 cat(&quot;True ATE:&quot;, mean(po_table$Causal_Effect), &quot;\\n&quot;) ## True ATE: -26.66667 cat(&quot;\\nThe naive estimate differs from the true ATE because treatment was\\n&quot;) ## ## The naive estimate differs from the true ATE because treatment was cat(&quot;not randomly assigned. Sicker patients received aspirin.\\n&quot;) ## not randomly assigned. Sicker patients received aspirin. Notice the gap: the naive comparison and the true ATE don’t match. This is because patients 1, 3, and 5 (who got aspirin) had higher baseline headache scores than patients 2, 4, and 6. The naive comparison confounds the drug’s effect with pre-existing differences between the groups, and this is exactly the problem that motivates everything in this course. 1.4 Causal Estimands and Identification Causal estimands are the quantities we aim to estimate to understand the effect of a treatment or intervention. The most common estimands include: Average Treatment Effect (ATE): Measures the average difference in outcomes if everyone received the treatment versus if no one did. \\[ATE = E[Y(1) - Y(0)]\\] Average Treatment Effect on the Treated (ATT): Measures the effect of treatment for those who actually received the treatment. \\[ATT = E[Y(1) - Y(0) \\mid W = 1]\\] Conditional Average Treatment Effect (CATE): Measures the treatment effect for subgroups defined by observed characteristics (e.g., older vs. younger patients). \\[CATE = E[Y(1) - Y(0) \\mid X = x]\\] Think of \\(X = x\\) where \\(X\\) is some form of age classification and \\(x\\) could be a value of the age classification, such as younger or older. Identification is the process of linking a causal estimand (like ATE) to observable data. Without valid identification, any estimates we produce may be biased or incorrect. One major challenge in causal inference is that we can never observe both potential outcomes for the same person, only the outcome under the actual treatment they received. This is the Fundamental Problem of Causal Inference. 1.5 Measuring Effects for Binary Outcomes Often in public health settings, we have to estimate the causal effect on a binary outcome such as survival. For example, if we are trying to measure the effect of a heart surgery on a patient’s survival status, we can say that the outcome \\(Y = 1\\) means that the patient died and \\(Y = 0\\) means that the patient survived. There are only two possible outcomes. In this case, we try to quantify or measure the causal effect using two main effect measures. Risk Difference (additive scale): \\[Pr[Y(1) = 1] - Pr[Y(0) = 1]\\] For example, if we estimated the heart surgery to have a risk difference of \\(-0.5\\), then on average, the surgery reduced the probability of death by 0.5. Risk Ratio (multiplicative scale): \\[\\frac{Pr[Y(1) = 1]}{Pr[Y(0) = 1]}\\] For example, if the heart surgery was estimated to have a risk ratio of 0.5, then the risk of death under surgery is half compared to the risk of death under no surgery. 1.5.1 Simulation: Heart Surgery and Survival Let’s put these measures to work. We simulate a study of 500 patients where we know the true potential outcomes for a heart surgery. Since this is a simulation, we can see both \\(Y(1)\\) and \\(Y(0)\\) for every patient, something we can never do in the real world. This lets us compute the true risk difference and risk ratio, and then see how the naive comparison (which ignores confounding) gets it wrong. set.seed(123) n &lt;- 500 # Simulate covariates age &lt;- rnorm(n, 60, 12) age &lt;- pmax(25, pmin(90, age)) health_score &lt;- rnorm(n, 60, 15) health_score &lt;- pmax(0, pmin(100, health_score)) # True potential outcome probabilities # Higher age and lower health → higher risk of death p0 &lt;- plogis(-0.98 + 0.034 * age - 0.036 * health_score) p1 &lt;- plogis(-0.98 + 0.034 * age - 0.036 * health_score - 0.51) # surgery helps # Generate BOTH potential outcomes (God&#39;s-eye view) Y0 &lt;- rbinom(n, 1, p0) Y1 &lt;- rbinom(n, 1, p1) # True causal quantities (using potential outcomes directly) true_rd &lt;- mean(Y1) - mean(Y0) true_rr &lt;- mean(Y1) / mean(Y0) cat(&quot;=== True Causal Effects (from potential outcomes) ===\\n&quot;) ## === True Causal Effects (from potential outcomes) === cat(&quot;Pr[Y(1)=1] (risk under surgery):&quot;, round(mean(Y1), 3), &quot;\\n&quot;) ## Pr[Y(1)=1] (risk under surgery): 0.202 cat(&quot;Pr[Y(0)=1] (risk without surgery):&quot;, round(mean(Y0), 3), &quot;\\n&quot;) ## Pr[Y(0)=1] (risk without surgery): 0.31 cat(&quot;Risk Difference:&quot;, round(true_rd, 3), &quot;\\n&quot;) ## Risk Difference: -0.108 cat(&quot;Risk Ratio:&quot;, round(true_rr, 3), &quot;\\n&quot;) ## Risk Ratio: 0.652 The surgery reduces the risk of death: the risk difference is negative and the risk ratio is below 1. But remember: in reality we never see both \\(Y(1)\\) and \\(Y(0)\\). Each patient walks only one path. 1.5.2 The Fundamental Problem in Action Now let’s see what happens when treatment is not randomly assigned. Sicker patients (older, lower health scores) are more likely to receive the surgery, just as in the real world. # Confounded treatment assignment: sicker patients get surgery W_prob &lt;- plogis(-1 + 0.03 * age - 0.03 * health_score) W &lt;- rbinom(n, 1, W_prob) # Observed outcome: we only see Y(1) for treated, Y(0) for untreated Y_obs &lt;- ifelse(W == 1, Y1, Y0) # Naive comparison: difference in observed means naive_rd &lt;- mean(Y_obs[W == 1]) - mean(Y_obs[W == 0]) naive_rr &lt;- mean(Y_obs[W == 1]) / mean(Y_obs[W == 0]) cat(&quot;=== Naive Comparison (what we observe) ===\\n&quot;) ## === Naive Comparison (what we observe) === cat(&quot;Death rate among treated:&quot;, round(mean(Y_obs[W == 1]), 3), &quot;\\n&quot;) ## Death rate among treated: 0.243 cat(&quot;Death rate among untreated:&quot;, round(mean(Y_obs[W == 0]), 3), &quot;\\n&quot;) ## Death rate among untreated: 0.269 cat(&quot;Naive Risk Difference:&quot;, round(naive_rd, 3), &quot;\\n&quot;) ## Naive Risk Difference: -0.027 cat(&quot;Naive Risk Ratio:&quot;, round(naive_rr, 3), &quot;\\n\\n&quot;) ## Naive Risk Ratio: 0.901 cat(&quot;=== Comparison ===\\n&quot;) ## === Comparison === cat(&quot;True Risk Difference:&quot;, round(true_rd, 3), &quot;\\n&quot;) ## True Risk Difference: -0.108 cat(&quot;Naive Risk Difference:&quot;, round(naive_rd, 3), &quot;\\n&quot;) ## Naive Risk Difference: -0.027 cat(&quot;Bias:&quot;, round(naive_rd - true_rd, 3), &quot;\\n&quot;) ## Bias: 0.081 The naive comparison is biased. It may even reverse the sign of the effect, making it look like the surgery hurts when it actually helps. This is exactly the HRT story from the slides: healthier women chose HRT, so the naive comparison confounded the treatment effect with pre-existing health differences. 1.5.3 Visualizing the Bias plot_df &lt;- data.frame( Measure = rep(c(&quot;Risk Difference&quot;, &quot;Risk Ratio&quot;), each = 2), Type = rep(c(&quot;True (potential outcomes)&quot;, &quot;Naive (observed)&quot;), 2), Value = c(true_rd, naive_rd, true_rr, naive_rr) ) ggplot(plot_df, aes(x = Type, y = Value, fill = Type)) + geom_col(width = 0.6, alpha = 0.8) + facet_wrap(~ Measure, scales = &quot;free_y&quot;) + scale_fill_manual(values = c(&quot;True (potential outcomes)&quot; = &quot;#2CA02C&quot;, &quot;Naive (observed)&quot; = &quot;#D62728&quot;)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + labs(title = &quot;True vs. Naive Effect Estimates Under Confounding&quot;, subtitle = &quot;Confounded treatment assignment biases both measures&quot;, y = &quot;Estimate&quot;, x = &quot;&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;, axis.text.x = element_text(size = 8)) This is the core challenge of causal inference with binary outcomes: without a valid study design, our estimates of risk differences and risk ratios can be severely misleading. Starting in Week 2, we will learn how randomization solves this problem, and in later weeks, we will learn estimation strategies for settings where randomization is not possible. Next week: Randomized Controlled Trials: why randomization solves the confounding problem, and we add Checklist Items 2-3 to our toolkit. "],["randomized-controlled-trials.html", "2 Randomized Controlled Trials Class materials Textbook Reading Supplementary Reading 2.1 Review: The Selection Bias Problem 2.2 Simulating a Randomized Experiment 2.3 Estimating the ATE 2.4 Repeated Experiments: The Sampling Distribution 2.5 Statistical vs. Clinical Significance 2.6 Exercise: The Oregon Health Insurance Experiment 2.7 Summary", " 2 Randomized Controlled Trials Class materials Slides: Module 2 Textbook Reading Hernán &amp; Robins, Causal Inference: What If – Chapters 1–2 Supplementary Reading Medical Research Council. (1948) “Streptomycin Treatment of Pulmonary Tuberculosis.” BMJ, 2(4582):769–782. Finkelstein A, et al. (2012) “The Oregon Health Insurance Experiment: Evidence from the First Year.” QJE, 127(3):1057–1106. ## Topics Covered Why randomization solves the selection bias problem Simulating a randomized experiment Estimating the Average Treatment Effect from an RCT Confidence intervals and uncertainty Statistical vs. clinical significance Critical reading exercise: the Oregon Health Insurance Experiment 2.1 Review: The Selection Bias Problem Last week, we saw that a naive comparison of treated and untreated groups can be misleading when treatment assignment is confounded, meaning that when the characteristics that drive someone to receive treatment also affect the outcome. Today we show why randomization eliminates this problem. Not all credible studies use researcher-controlled randomization. Sometimes nature provides “as-if random” assignment through lotteries (e.g., the Vietnam draft), arbitrary cutoffs (e.g., the legal drinking age at 21), or geographic boundaries. These quasi-experiments sit between full randomization and pure self-selection on the credibility spectrum. We will formalize these designs later in the course; the OHIE exercise at the end of this page is one such example. Checklist Items 2-3. As you work through this code, keep two questions in mind: How was treatment assigned? (Item 2) and Is the comparison fair? (Item 3). When treatment is randomized, the answer to both questions is strong, and the code below shows you exactly why. 2.1.1 Quick Demonstration: Confounded vs. Randomized Assignment We start with a simple comparison. We simulate 2,000 patients where a blood pressure drug truly lowers systolic blood pressure (SBP) by 10 mmHg on average. In the first scenario, sicker patients are more likely to receive the drug (confounded). In the second, treatment is assigned by a coin flip (randomized). set.seed(42) n &lt;- 2000 # Covariates age &lt;- runif(n, 25, 75) baseline_sbp &lt;- 90 + 0.5 * age + rnorm(n, sd = 10) # Potential outcomes (true ATE = -10) y0 &lt;- baseline_sbp + rnorm(n, sd = 5) y1 &lt;- y0 - 10 # --- Scenario 1: Confounded assignment (sicker patients get drug) --- prob_treat_conf &lt;- plogis(-2 + 0.03 * age + 0.02 * baseline_sbp) W_conf &lt;- rbinom(n, 1, prob_treat_conf) y_conf &lt;- ifelse(W_conf == 1, y1, y0) naive_conf &lt;- mean(y_conf[W_conf == 1]) - mean(y_conf[W_conf == 0]) # --- Scenario 2: Randomized assignment (coin flip) --- W_rand &lt;- rbinom(n, 1, 0.5) y_rand &lt;- ifelse(W_rand == 1, y1, y0) naive_rand &lt;- mean(y_rand[W_rand == 1]) - mean(y_rand[W_rand == 0]) cat(&quot;True ATE:&quot;, -10, &quot;\\n\\n&quot;) ## True ATE: -10 cat(&quot;Confounded assignment:\\n&quot;) ## Confounded assignment: cat(&quot; Estimated ATE (naive):&quot;, round(naive_conf, 2), &quot;\\n&quot;) ## Estimated ATE (naive): -3.62 cat(&quot; Bias:&quot;, round(naive_conf - (-10), 2), &quot;\\n\\n&quot;) ## Bias: 6.38 cat(&quot;Randomized assignment:\\n&quot;) ## Randomized assignment: cat(&quot; Estimated ATE (naive):&quot;, round(naive_rand, 2), &quot;\\n&quot;) ## Estimated ATE (naive): -9.93 cat(&quot; Bias:&quot;, round(naive_rand - (-10), 2), &quot;\\n&quot;) ## Bias: 0.07 Under confounded assignment, the naive estimate is biased because sicker patients received the drug, so the treated group had higher SBP to begin with. Under randomization, the difference in means is close to the true ATE of \\(-10\\) because the two groups are comparable. 2.2 Simulating a Randomized Experiment Let’s build a full RCT simulation step by step. We simulate a clinical trial testing whether a new blood pressure drug lowers SBP compared to placebo. 2.2.1 Setting Up the Population set.seed(2024) n &lt;- 2000 # Generate patient characteristics age &lt;- runif(n, 25, 75) female &lt;- rbinom(n, 1, 0.5) smoker &lt;- rbinom(n, 1, 0.22) bmi &lt;- rnorm(n, mean = 27, sd = 4) diabetes &lt;- rbinom(n, 1, 0.12) # Baseline SBP depends on covariates baseline_sbp &lt;- 80 + 0.5 * age + 2 * smoker + 0.3 * bmi + 5 * diabetes + rnorm(n, sd = 8) df &lt;- data.frame(age, female, smoker, bmi, diabetes, baseline_sbp) head(df) ## age female smoker bmi diabetes baseline_sbp ## 1 66.84713 0 1 27.00611 0 102.0426 ## 2 41.04338 1 0 28.14770 0 103.7882 ## 3 59.01817 0 0 29.11518 0 117.3457 ## 4 59.90866 0 0 26.32628 0 122.8278 ## 5 47.85046 0 0 27.66546 0 113.8976 ## 6 60.07102 1 0 30.44778 0 126.6579 2.2.2 Randomization: Treatment Assignment In an RCT, treatment is assigned independently of patient characteristics. This is the key that ensures exchangeability: \\(Y(0), Y(1) \\perp W\\). # Simple randomization (coin flip) df$W &lt;- rbinom(n, 1, 0.5) cat(&quot;Treatment group:&quot;, sum(df$W), &quot;patients\\n&quot;) ## Treatment group: 979 patients cat(&quot;Control group:&quot;, sum(1 - df$W), &quot;patients\\n&quot;) ## Control group: 1021 patients 2.2.3 Checking Balance: Does Randomization Work? A crucial step in evaluating any RCT is checking whether baseline characteristics are balanced across treatment and control groups. This is what Table 1 shows in a published RCT paper. balance_check &lt;- df %&gt;% group_by(W) %&gt;% summarize( mean_age = mean(age), pct_female = mean(female) * 100, pct_smoker = mean(smoker) * 100, mean_bmi = mean(bmi), pct_diabetes = mean(diabetes) * 100, mean_sbp = mean(baseline_sbp), .groups = &quot;drop&quot; ) # Format for display balance_display &lt;- data.frame( Variable = c(&quot;Age (mean)&quot;, &quot;Female (%)&quot;, &quot;Smoker (%)&quot;, &quot;BMI (mean)&quot;, &quot;Diabetes (%)&quot;, &quot;Baseline SBP (mean)&quot;), Control = round(as.numeric(balance_check[balance_check$W == 0, -1]), 1), Treatment = round(as.numeric(balance_check[balance_check$W == 1, -1]), 1) ) balance_display$Difference &lt;- balance_display$Treatment - balance_display$Control knitr::kable(balance_display, caption = &quot;Table 1: Baseline Characteristics by Treatment Group&quot;) Table 2.1: Table 1: Baseline Characteristics by Treatment Group Variable Control Treatment Difference Age (mean) 50.0 50.1 0.1 Female (%) 49.1 51.1 2.0 Smoker (%) 22.2 22.9 0.7 BMI (mean) 27.0 26.9 -0.1 Diabetes (%) 13.1 12.7 -0.4 Baseline SBP (mean) 114.0 113.6 -0.4 The differences should be small, not because of any adjustment, but because randomization makes the groups comparable. With 1,000 patients per arm, we expect very good balance. 2.2.4 Visualizing Balance ggplot(df, aes(x = age, fill = factor(W, labels = c(&quot;Control&quot;, &quot;Treatment&quot;)))) + geom_density(alpha = 0.5) + scale_fill_manual(values = c(&quot;Control&quot; = &quot;#D62728&quot;, &quot;Treatment&quot; = &quot;#1F77B4&quot;), name = &quot;Group&quot;) + labs(title = &quot;Age Distribution by Treatment Group&quot;, subtitle = &quot;Under randomization, distributions should overlap substantially&quot;, x = &quot;Age&quot;, y = &quot;Density&quot;) + theme_minimal() Checklist Item 3 in practice: When you read an RCT, look at Table 1. If the baseline characteristics are similar across groups, the comparison is likely fair. Large imbalances, especially in a randomized study, suggest something may have gone wrong. 2.3 Estimating the ATE 2.3.1 Generating Outcomes # True treatment effect: drug lowers SBP by 10 mmHg on average true_ate &lt;- -10 # Potential outcomes df$y0 &lt;- baseline_sbp + rnorm(n, sd = 5) # outcome without drug df$y1 &lt;- df$y0 + true_ate # outcome with drug # Observed outcome (we only see one per person) df$y_obs &lt;- ifelse(df$W == 1, df$y1, df$y0) 2.3.2 The Simple Estimator: Difference in Means In an RCT, the ATE is estimated by the difference in sample means. No regression or matching needed. \\[\\widehat{ATE} = \\bar{Y}_1 - \\bar{Y}_0 = \\frac{1}{N_1}\\sum_{i:W_i=1} Y_i^{obs} - \\frac{1}{N_0}\\sum_{i:W_i=0} Y_i^{obs}\\] y_bar_1 &lt;- mean(df$y_obs[df$W == 1]) y_bar_0 &lt;- mean(df$y_obs[df$W == 0]) ate_hat &lt;- y_bar_1 - y_bar_0 cat(&quot;Mean outcome (Treatment):&quot;, round(y_bar_1, 2), &quot;\\n&quot;) ## Mean outcome (Treatment): 103.89 cat(&quot;Mean outcome (Control):&quot;, round(y_bar_0, 2), &quot;\\n&quot;) ## Mean outcome (Control): 113.67 cat(&quot;Estimated ATE:&quot;, round(ate_hat, 2), &quot;\\n&quot;) ## Estimated ATE: -9.78 cat(&quot;True ATE:&quot;, true_ate, &quot;\\n&quot;) ## True ATE: -10 cat(&quot;Estimation error:&quot;, round(ate_hat - true_ate, 2), &quot;\\n&quot;) ## Estimation error: 0.22 The estimate is close to the true value of \\(-10\\). This works because randomization ensures exchangeability: the treated and control groups are comparable, so the selection bias term equals zero. 2.3.3 Confidence Intervals A point estimate alone is not enough. We need to quantify uncertainty. # Standard error of the difference in means n1 &lt;- sum(df$W == 1) n0 &lt;- sum(df$W == 0) s1 &lt;- sd(df$y_obs[df$W == 1]) s0 &lt;- sd(df$y_obs[df$W == 0]) se &lt;- sqrt(s1^2 / n1 + s0^2 / n0) # 95% confidence interval ci_lower &lt;- ate_hat - 1.96 * se ci_upper &lt;- ate_hat + 1.96 * se cat(&quot;Estimated ATE:&quot;, round(ate_hat, 2), &quot;\\n&quot;) ## Estimated ATE: -9.78 cat(&quot;Standard Error:&quot;, round(se, 2), &quot;\\n&quot;) ## Standard Error: 0.54 cat(&quot;95% CI: [&quot;, round(ci_lower, 2), &quot;,&quot;, round(ci_upper, 2), &quot;]\\n\\n&quot;) ## 95% CI: [ -10.83 , -8.72 ] if (ci_lower &gt; 0 | ci_upper &lt; 0) { cat(&quot;The CI does NOT include zero → evidence of a non-zero effect.\\n&quot;) } else { cat(&quot;The CI includes zero → we cannot rule out no effect.\\n&quot;) } ## The CI does NOT include zero → evidence of a non-zero effect. 2.3.4 Using a t-test (Equivalent Approach) The difference-in-means test is exactly a two-sample t-test. R makes this easy: t_result &lt;- t.test(y_obs ~ W, data = df) print(t_result) ## ## Welch Two Sample t-test ## ## data: y_obs by W ## t = 18.174, df = 1990.4, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## 8.724008 10.834577 ## sample estimates: ## mean in group 0 mean in group 1 ## 113.6710 103.8917 The p-value and confidence interval from t.test() match our manual calculation. Under randomization, this simple test is all you need. 2.3.5 Visualizing the Treatment Effect ggplot(df, aes(x = factor(W, labels = c(&quot;Control&quot;, &quot;Treatment&quot;)), y = y_obs, fill = factor(W, labels = c(&quot;Control&quot;, &quot;Treatment&quot;)))) + geom_boxplot(alpha = 0.7, outlier.alpha = 0.3) + scale_fill_manual(values = c(&quot;Control&quot; = &quot;#D62728&quot;, &quot;Treatment&quot; = &quot;#1F77B4&quot;)) + labs(title = &quot;Observed Outcomes by Treatment Group&quot;, subtitle = paste0(&quot;Estimated ATE = &quot;, round(ate_hat, 2), &quot; mmHg (true = &quot;, true_ate, &quot;)&quot;), x = &quot;&quot;, y = &quot;Systolic Blood Pressure (mmHg)&quot;) + theme_minimal() + guides(fill = &quot;none&quot;) 2.4 Repeated Experiments: The Sampling Distribution A single RCT gives one estimate. But if we could repeat the experiment many times, the estimates would form a distribution centered on the true ATE. This is the idea behind the sampling distribution. set.seed(2024) n_sims &lt;- 1000 estimates &lt;- numeric(n_sims) for (s in 1:n_sims) { # Rerandomize treatment W_sim &lt;- rbinom(n, 1, 0.5) y_sim &lt;- ifelse(W_sim == 1, df$y1, df$y0) estimates[s] &lt;- mean(y_sim[W_sim == 1]) - mean(y_sim[W_sim == 0]) } ggplot(data.frame(est = estimates), aes(x = est)) + geom_histogram(bins = 40, fill = &quot;#1F77B4&quot;, alpha = 0.7, color = &quot;white&quot;) + geom_vline(xintercept = true_ate, color = &quot;red&quot;, linewidth = 1.2, linetype = &quot;dashed&quot;) + geom_vline(xintercept = mean(estimates), color = &quot;#2CA02C&quot;, linewidth = 1) + labs(title = &quot;Sampling Distribution of the ATE Estimator&quot;, subtitle = paste0(&quot;1,000 re-randomizations | Mean of estimates: &quot;, round(mean(estimates), 2), &quot; | True ATE: &quot;, true_ate), x = &quot;Estimated ATE&quot;, y = &quot;Count&quot;) + theme_minimal() cat(&quot;Mean of 1,000 estimates:&quot;, round(mean(estimates), 2), &quot;\\n&quot;) ## Mean of 1,000 estimates: -10 cat(&quot;SD of estimates:&quot;, round(sd(estimates), 2), &quot;\\n&quot;) ## SD of estimates: 0.66 cat(&quot;True ATE:&quot;, true_ate, &quot;\\n&quot;) ## True ATE: -10 The histogram is centered on \\(-10\\), confirming the estimator is unbiased. The spread of the distribution is the standard error, which decreases with larger sample sizes. 2.5 Statistical vs. Clinical Significance A result can be statistically significant yet clinically irrelevant, or clinically important but statistically non-significant. The next simulation illustrates both scenarios. set.seed(99) # Scenario 1: Large sample, tiny true effect (0.5 mmHg) n_large &lt;- 10000 W_large &lt;- rbinom(n_large, 1, 0.5) y0_large &lt;- rnorm(n_large, mean = 130, sd = 15) y1_large &lt;- y0_large - 0.5 # tiny effect y_large &lt;- ifelse(W_large == 1, y1_large, y0_large) t1 &lt;- t.test(y_large[W_large == 1], y_large[W_large == 0]) cat(&quot;=== Scenario 1: Large sample, tiny effect ===\\n&quot;) ## === Scenario 1: Large sample, tiny effect === cat(&quot;True effect: -0.5 mmHg\\n&quot;) ## True effect: -0.5 mmHg cat(&quot;Estimated effect:&quot;, round(t1$estimate[1] - t1$estimate[2], 2), &quot;mmHg\\n&quot;) ## Estimated effect: -0.52 mmHg cat(&quot;P-value:&quot;, format.pval(t1$p.value, digits = 3), &quot;\\n&quot;) ## P-value: 0.0805 cat(&quot;Statistically significant?&quot;, ifelse(t1$p.value &lt; 0.05, &quot;YES&quot;, &quot;NO&quot;), &quot;\\n&quot;) ## Statistically significant? NO cat(&quot;Clinically meaningful? Probably not. 0.5 mmHg is negligible.\\n\\n&quot;) ## Clinically meaningful? Probably not. 0.5 mmHg is negligible. # Scenario 2: Small sample, large true effect (15 mmHg) n_small &lt;- 40 W_small &lt;- rbinom(n_small, 1, 0.5) y0_small &lt;- rnorm(n_small, mean = 130, sd = 15) y1_small &lt;- y0_small - 15 # large effect y_small &lt;- ifelse(W_small == 1, y1_small, y0_small) t2 &lt;- t.test(y_small[W_small == 1], y_small[W_small == 0]) cat(&quot;=== Scenario 2: Small sample, large effect ===\\n&quot;) ## === Scenario 2: Small sample, large effect === cat(&quot;True effect: -15 mmHg\\n&quot;) ## True effect: -15 mmHg cat(&quot;Estimated effect:&quot;, round(t2$estimate[1] - t2$estimate[2], 2), &quot;mmHg\\n&quot;) ## Estimated effect: -15.88 mmHg cat(&quot;P-value:&quot;, format.pval(t2$p.value, digits = 3), &quot;\\n&quot;) ## P-value: 0.00131 cat(&quot;Statistically significant?&quot;, ifelse(t2$p.value &lt; 0.05, &quot;YES&quot;, &quot;NO&quot;), &quot;\\n&quot;) ## Statistically significant? YES cat(&quot;Clinically meaningful? Yes. 15 mmHg is a substantial reduction.\\n&quot;) ## Clinically meaningful? Yes. 15 mmHg is a substantial reduction. Takeaway: Always ask both questions: (1) Is the effect statistically distinguishable from zero? (2) Is the effect large enough to matter for patients or policy? 2.6 Exercise: The Oregon Health Insurance Experiment The Oregon Health Insurance Experiment (OHIE) is one of the most influential randomized studies in health policy. In 2008, Oregon used a lottery to randomly select who could apply for Medicaid, creating a natural experiment on the effect of health insurance. 2.6.1 Background Treatment (\\(W\\)): Winning the lottery (gaining access to apply for Medicaid) Control (\\(W = 0\\)): Not selected in the lottery Outcomes (\\(Y\\)): Healthcare utilization, health outcomes, financial strain Population: Low-income uninsured adults in Oregon Checklist Items 1-3: Item 1: The causal question is clear: does gaining access to Medicaid improve health outcomes? Item 2: Treatment was assigned by lottery (random). Strong. Item 3: Lottery winners and losers should be comparable at baseline, so the comparison is fair. 2.6.2 Loading the Data # Load the OHIE data # The data file should be in your working directory # Uncomment and modify the path below once the data are provided # # ohie &lt;- read.csv(&quot;ohie_data.csv&quot;) # head(ohie) # For now, we simulate OHIE-like data to illustrate the analysis set.seed(2008) n_ohie &lt;- 10000 # Lottery assignment (randomized) lottery_win &lt;- rbinom(n_ohie, 1, 0.5) # Baseline characteristics (should be balanced by randomization) age &lt;- runif(n_ohie, 19, 64) female &lt;- rbinom(n_ohie, 1, 0.55) english &lt;- rbinom(n_ohie, 1, 0.85) # Outcomes: healthcare utilization (binary: had a doctor visit in past year) # True effect of lottery win on doctor visit: +12 percentage points prob_visit &lt;- plogis(-0.5 + 0.01 * age + 0.2 * female + 0.12 * lottery_win) doctor_visit &lt;- rbinom(n_ohie, 1, prob_visit) # Financial strain: had any medical debt (binary) # True effect: -8 percentage points prob_debt &lt;- plogis(0.1 + 0.005 * age - 0.1 * female - 0.08 * lottery_win) medical_debt &lt;- rbinom(n_ohie, 1, prob_debt) ohie &lt;- data.frame(lottery_win, age, female, english, doctor_visit, medical_debt) 2.6.3 Step 1: Check Balance (Table 1) Before estimating effects, verify that the lottery created comparable groups. ohie_balance &lt;- ohie %&gt;% group_by(lottery_win) %&gt;% summarize( n = n(), mean_age = mean(age), pct_female = mean(female) * 100, pct_english = mean(english) * 100, .groups = &quot;drop&quot; ) balance_tbl &lt;- data.frame( Variable = c(&quot;N&quot;, &quot;Age (mean)&quot;, &quot;Female (%)&quot;, &quot;English speaker (%)&quot;), `Non-selected` = c(ohie_balance$n[1], round(ohie_balance$mean_age[1], 1), round(ohie_balance$pct_female[1], 1), round(ohie_balance$pct_english[1], 1)), `Lottery winner` = c(ohie_balance$n[2], round(ohie_balance$mean_age[2], 1), round(ohie_balance$pct_female[2], 1), round(ohie_balance$pct_english[2], 1)) ) knitr::kable(balance_tbl, caption = &quot;Table 1: Baseline Characteristics by Lottery Status&quot;) Table 2.2: Table 1: Baseline Characteristics by Lottery Status Variable Non.selected Lottery.winner N 5012.0 4988.0 Age (mean) 41.8 41.3 Female (%) 55.6 55.1 English speaker (%) 84.8 84.2 Because assignment was random, the groups should look similar. If you see large imbalances, it would raise questions about the randomization. 2.6.4 Step 2: Estimate the Intent-to-Treat (ITT) Effect The intent-to-treat analysis compares outcomes based on assignment (lottery win vs. loss), regardless of whether winners actually enrolled in Medicaid. This preserves the randomization. # Doctor visits itt_visit &lt;- mean(ohie$doctor_visit[ohie$lottery_win == 1]) - mean(ohie$doctor_visit[ohie$lottery_win == 0]) t_visit &lt;- t.test(doctor_visit ~ lottery_win, data = ohie) cat(&quot;=== ITT: Doctor Visit in Past Year ===\\n&quot;) ## === ITT: Doctor Visit in Past Year === cat(&quot;Lottery winners:&quot;, round(mean(ohie$doctor_visit[ohie$lottery_win == 1]) * 100, 1), &quot;%\\n&quot;) ## Lottery winners: 53.4 % cat(&quot;Non-selected:&quot;, round(mean(ohie$doctor_visit[ohie$lottery_win == 0]) * 100, 1), &quot;%\\n&quot;) ## Non-selected: 50.8 % cat(&quot;ITT effect:&quot;, round(itt_visit * 100, 1), &quot;percentage points\\n&quot;) ## ITT effect: 2.6 percentage points cat(&quot;95% CI: [&quot;, round(t_visit$conf.int[1] * 100, 1), &quot;,&quot;, round(t_visit$conf.int[2] * 100, 1), &quot;] pp\\n&quot;) ## 95% CI: [ -4.5 , -0.6 ] pp cat(&quot;P-value:&quot;, format.pval(t_visit$p.value, digits = 3), &quot;\\n\\n&quot;) ## P-value: 0.0101 # Medical debt itt_debt &lt;- mean(ohie$medical_debt[ohie$lottery_win == 1]) - mean(ohie$medical_debt[ohie$lottery_win == 0]) t_debt &lt;- t.test(medical_debt ~ lottery_win, data = ohie) cat(&quot;=== ITT: Any Medical Debt ===\\n&quot;) ## === ITT: Any Medical Debt === cat(&quot;Lottery winners:&quot;, round(mean(ohie$medical_debt[ohie$lottery_win == 1]) * 100, 1), &quot;%\\n&quot;) ## Lottery winners: 54.4 % cat(&quot;Non-selected:&quot;, round(mean(ohie$medical_debt[ohie$lottery_win == 0]) * 100, 1), &quot;%\\n&quot;) ## Non-selected: 55.5 % cat(&quot;ITT effect:&quot;, round(itt_debt * 100, 1), &quot;percentage points\\n&quot;) ## ITT effect: -1.2 percentage points cat(&quot;95% CI: [&quot;, round(t_debt$conf.int[1] * 100, 1), &quot;,&quot;, round(t_debt$conf.int[2] * 100, 1), &quot;] pp\\n&quot;) ## 95% CI: [ -0.8 , 3.1 ] pp cat(&quot;P-value:&quot;, format.pval(t_debt$p.value, digits = 3), &quot;\\n&quot;) ## P-value: 0.245 2.6.5 Step 3: Visualize the Results results &lt;- data.frame( Outcome = c(&quot;Doctor Visit&quot;, &quot;Medical Debt&quot;), ITT = c(itt_visit, itt_debt), Lower = c(t_visit$conf.int[1], t_debt$conf.int[1]), Upper = c(t_visit$conf.int[2], t_debt$conf.int[2]) ) ggplot(results, aes(x = Outcome, y = ITT)) + geom_point(size = 4, color = &quot;#1F77B4&quot;) + geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.15, linewidth = 1, color = &quot;#1F77B4&quot;) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + labs(title = &quot;Oregon HIE: Intent-to-Treat Effects&quot;, subtitle = &quot;Difference in means (lottery winners vs. non-selected) with 95% CI&quot;, y = &quot;Effect (proportion)&quot;, x = &quot;&quot;) + coord_flip() + theme_minimal() 2.6.6 A Nuance: ITT vs. the Effect of Having Insurance Not all lottery winners actually enrolled in Medicaid (only about 25% in the real OHIE). This means the ITT estimates the effect of winning the lottery (access to Medicaid), not the effect of having Medicaid (actual coverage). The ITT is still a valid causal estimate, but it answers a slightly different question. We will learn how to handle this kind of noncompliance in Week 7 using instrumental variables. For now, the key point is: always analyze by assignment to preserve the integrity of the randomization. 2.7 Summary This week’s programming exercises demonstrated three key ideas: Randomization eliminates selection bias: When treatment is randomly assigned, the naive difference in means is an unbiased estimator of the ATE. No regression or adjustment needed. Balance is the observable consequence of exchangeability: Checking Table 1 (baseline characteristics by group) is the practical way to verify that the comparison is fair (Checklist Item 3). The ITT analysis preserves the randomization: In the OHIE, comparing lottery winners to non-selected individuals gives a clean causal estimate of the effect of access to Medicaid. Our Causal Credibility Checklist now has three items: \\(\\boxtimes\\) Item 1: What is the causal question? (Week 1) \\(\\boxtimes\\) Item 2: How was treatment assigned? (Week 2) \\(\\boxtimes\\) Item 3: Is the comparison fair? (Week 2) Next week: What happens when we can’t randomize? We move to observational studies and add Checklist Item 4: could confounding explain the result? "],["observational-studies.html", "3 Observational Studies Class materials Textbook reading Supplementary reading Topics covered 3.1 The challenge of confounding in public health and medical research 3.2 Exchangeability, positivity, and SUTVA 3.3 Effect Identification in Observational Studies", " 3 Observational Studies Class materials Slides: Module 3 Recording: Module 3, Part 1 Recording: Module 3, Part 2 Textbook reading Hernán &amp; Robins, Causal Inference: What If – Chapter 3 Supplementary reading Greenland, S. (2003). Quantifying biases in causal models: classical confounding vs collider-stratification bias. Epidemiology, 14(3), 300–306.  Additional DAG exercises provided in class Topics covered The challenge of confounding in public health and medical research Exchangeability, positivity, and consistency Effect identification in observational studies Critical reading exercise: evaluating a published observational study 3.1 The challenge of confounding in public health and medical research Confounding is a major challenge in public health and medical research because it can create misleading associations between exposures and outcomes. A confounder is a third variable that is associated with both the exposure and the outcome, potentially distorting the true causal relationship. For example, if we observe that people who carry lighters tend to have higher rates of lung cancer, we might wrongly conclude that carrying a lighter causes cancer. In reality, smoking is the confounding variable: smokers are more likely to carry lighters and also more likely to develop lung cancer. Without properly adjusting for confounders, studies risk producing biased estimates, leading to incorrect conclusions about risk factors, treatments, or interventions. Addressing confounding is crucial but not always straightforward. Methods such as stratification, multivariable regression, propensity score matching, and randomized controlled trials (RCTs) are commonly used to try to adjust for or eliminate confounding effects. However, identifying all relevant confounders can be difficult, especially when dealing with observational data where randomization is not possible. Unmeasured or unknown confounders remain a constant threat to validity. Therefore, careful study design, domain knowledge, and sensitivity analyses are essential to minimize the impact of confounding and ensure more reliable and actionable public health research findings. Example Setup Let’s say we want to study the effect of Exercise (X) on Heart Health (Y), but there’s a Genetic Factor (Z) that causes both Exercise and Heart Health. In this case, Z is a confounder, and we should adjust for it. In this simulation we will have two models: a naive model and an adjusted model. The naive model will only regression heart health on exercise. The adjusted model will regress heart health on exercise and genetic factors, which controls for genetic factor being a confounder. Since this is simulated example, we know that the true effect of exercise on heart health is 0.8. We will see in this example that the estimated causal effect coming from the adjusted model is better than the naive model. n &lt;- 2000 genetics &lt;- rnorm(n) exercise &lt;- 0.6 * genetics + rnorm(n) heart_health &lt;- 0.8 * exercise + 0.5 * genetics + rnorm(n) df &lt;- data.frame(heart_health, exercise, genetics) model_naive &lt;- lm(heart_health ~ exercise, data = df) summary(model_naive)$coefficients[&quot;exercise&quot;, ] ## Estimate Std. Error t value Pr(&gt;|t|) ## 1.018337 0.020338 50.070657 0.000000 model_adjusted &lt;- lm(heart_health ~ exercise + genetics, data = df) summary(model_adjusted)$coefficients[&quot;exercise&quot;, ] ## Estimate Std. Error t value Pr(&gt;|t|) ## 8.080949e-01 2.126973e-02 3.799272e+01 3.610292e-238 # library(ggplot2) naive_estimate &lt;- summary(model_naive)$coefficients[&quot;exercise&quot;, &quot;Estimate&quot;] adjusted_estimate &lt;- summary(model_adjusted)$coefficients[&quot;exercise&quot;, &quot;Estimate&quot;] estimates &lt;- data.frame( Model = c(&quot;Naive&quot;, &quot;Adjusted&quot;), Estimate = c(naive_estimate, adjusted_estimate) ) ggplot(estimates, aes(x = Model, y = Estimate, fill = Model)) + geom_col(width = 0.5) + labs(title = &quot;Comparison of Naive vs Adjusted Estimates&quot;, y = &quot;Estimated Effect of Exercise&quot;, x = &quot;&quot;) + geom_hline(yintercept = 0.8, linetype = 2) + annotate(&quot;text&quot;, x = 2.45, y = 0.85, label = &quot;True effect&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) In this simulation, we model a situation where Genetics (Z) is a confounder that influences both Exercise (X) and Heart Health (Y). The naive model, which regresses Heart Health on Exercise without adjusting for Genetics, gives a biased estimate of the effect of Exercise (1.05). This happens because part of the observed association is actually due to Genetics, not Exercise itself. When we adjust for Genetics in the second model, the estimate of Exercise’s effect (0.84) becomes more accurate, isolating its true relationship with Heart Health. This is shown in how the bar for the adjusted graph is closer to the dotted black line, which represents the true causal effect of exercise (0.8). This example highlights how failing to account for confounding can lead researchers to overstate or misinterpret causal effects in public health and medical studies. 3.2 Exchangeability, positivity, and SUTVA In causal inference, particularly when analyzing observational data, three critical assumptions must hold for estimates to reflect true causal relationships: exchangeability, positivity, and SUTVA. These assumptions ensure that the comparisons we make between groups are valid and that the effects we estimate correspond to real-world interventions. Without them, causal conclusions can be biased or entirely invalid. Exchangeability means that after adjusting for confounders, the treatment and comparison groups are similar in all relevant ways except for the exposure itself. In potential outcomes notation, this is written as: \\[ (Y(0), Y(1)) \\perp W|X \\] Positivity means that every individual has a nonzero probability of receiving each level of the exposure, regardless of their confounder values. In the notation, the “for any \\(x \\in \\mathcal{X}\\)” is saying that within in each group \\(x\\) that the random variable \\(X\\) can take, the unit has a nonzero chance to receive treatment (and control). \\[ 0 &lt; Pr(W = 1 \\ | \\ X = x) &lt; 1 \\quad \\text{for any } x \\in \\mathcal{X} \\] Positivity is not always an assumption that is satisfied. For example, let’s say you are trying to estimate the effect of a new drug. You want to control for the patient’s sex assigned at birth (male or female) to try to estimate the causal effect. However, the drug is never given to females. Then this assumption is violated. Stable Unit Treatment Value Assumption (SUTVA) is our last important assumption. There are two parts to SUTVA: Consistency means that the observed outcome under the observed treatment status of unit \\(i\\) is the same as the potential outcome under the potential treatment status of unit \\(i\\). This usually occurs when there is a single well-defined treatment, such as a drug with a specific dosage. Consistency can be violated if there are multiple versions of the treatment, usually because it is not well defined. For example, there could be a treatment that gives 50 mg of the drug and another version that gives 100 mg of the drug. Here, consistency will be violated. In potential outcomes notation, this is: \\[ Y = (W)Y(1) + (1-W)Y(0) \\] No-interference means that the potential outcomes of unit \\(i\\) are independent of the treatments other units receive. An example of when there is interference is when we are trying to test a vaccine’s effectiveness of preventing measles for an individual. Let’s say in our study, Falco receives the vaccine, and his friend Chad receives a control. Falco and Chad both interact regularly. Assume the vaccine truly reduces the risk of catching measles. Since Falco received the vaccine, he has a lower risk of catching measles, which lowers Chad’s risk of catching measles, even though Chad never received the vaccine. So Chad’s potential risk of contracting measles is a function of whether he receives the vaccine or not and whether Falco receives the vaccine or not. Thus, the no-interference assumption is violated in this case. In potential outcomes notation, we say the potential outcome for unit \\(i\\) is not a function of the treatment status of another unit \\(j\\): \\[ Y_i(W_i) = Y_i(W_i,W_j) \\quad \\text{for all } j \\neq i \\] In our previous simulation studying exercise and heart health, adjusting for genetics aimed to restore exchangeability by balancing genetic differences between individuals with different exercise levels. Positivity was satisfied because individuals at all levels of genetics still varied in how much they exercised. Consistency was assumed because the way we measured exercise and heart health accurately reflected the underlying causal relationship. Together, these assumptions allowed us to interpret the adjusted effect of exercise on heart health as a causal effect. 3.3 Effect Identification in Observational Studies In observational studies, identifying causal effects is challenging because researchers do not control exposure assignments. Unlike randomized controlled trials, individuals self-select into exposure groups, leading to potential confounding. Effect identification requires careful strategies to mimic the conditions of randomization and ensure that observed associations reflect true causal relationships rather than biases from confounding or selection. Confounding control: Adjust for confounders through methods like regression, stratification, matching, or weighting to approximate randomization. Assumptions: Rely on assumptions like exchangeability, positivity, and consistency to justify causal interpretation. Sensitivity analysis: Explore how robust the estimated effect is to potential unmeasured confounding. In our simulation of exercise and heart health, we identified the causal effect of exercise by adjusting for the confounding effect of genetics. Without randomization, genetics could have biased the relationship between exercise and health outcomes. By including genetics as a covariate in our model, we attempted to recreate the conditions needed for causal identification in an observational setting, relying on the assumptions of exchangeability, positivity, and consistency to interpret the adjusted exercise effect as causal. "],["effect-modification-and-interaction.html", "4 Effect Modification and Interaction Class materials Textbook reading Supplementary reading Topics covered 4.1 Effect Modification and Adjustment Methods 4.2 Interaction 4.3 Identifying Interaction 4.4 Effect Modification vs Interaction", " 4 Effect Modification and Interaction Class materials Slides: Module 4 Recording: Module 4, Part 1 Recording: Module 4, Part 2 Textbook reading Hernán &amp; Robins, Causal Inference: What If – Chapters 4–5 Supplementary reading Freedman, D. A. (2008). On types of scientific inquiry: The role of RCTs in health policy. Journal of the Royal Statistical Society: Series A, 171(2), 359–385.  Examples of quasi-experiments from public health and education Topics covered Effect modification and adjustment methods Interaction Identifying interaction Effect modification vs interaction Critical reading exercise: evaluating effect modification and interaction in studies 4.1 Effect Modification and Adjustment Methods Effect modification occurs when the effect of an exposure on an outcome differs depending on the level of another variable. Unlike confounding, which biases the estimated effect, effect modification reflects a real variation in the causal effect across different subgroups. Recognizing effect modification is important because it can reveal that a treatment or exposure is beneficial for some groups but not for others. Adjustment methods like stratification or including interaction terms in regression models help detect and describe effect modification rather than “control” it away. When we study effect modification, we are asking causal questions such as “What the effect of a treatment for group 1 vs the effect for group 2?” Adjustment methods typically aim to control for confounding, but they can also be used to detect effect modification when interaction terms are included. When effect modification is present, a single summary effect estimate (like an overall average) can be misleading. Instead, researchers often report subgroup-specific effects. Careful modeling and interpretation are necessary to distinguish between true effect modification and residual confounding. Simulation to Demonstrate Effect Modification Let’s say we are interested in understanding the effect of new drug on a person’s VO2 max for young vs old people. For example, this drug might be more effective for young people. VO2 max is the maximum amount of oxygen your body can utilize during exercise. VO2 max is measured in mL per kg of body weight per minute. In this simulation, the drug increases VO2 max by 4 mL/kg/min for old people, and 7.5 mL/kg/min for young people. set.seed(123) n &lt;- 2000 treatment &lt;- rbinom(n, 1, 0.5) # drug age_group &lt;- rbinom(n, 1, 0.5) # 1 = young, 0 = old baseline &lt;- 35 # baseline VO2 max is 35 mL/kg/min vo2_max &lt;- baseline + 4 * treatment + 0.5 * age_group + 3.5 * (treatment * age_group) + rnorm(n, mean = 0, sd = 1) df &lt;- data.frame(vo2_max, treatment, age_group) naive_model &lt;- lm(vo2_max ~ treatment + age_group, data = df) adjusted_model &lt;- lm(vo2_max ~ treatment + age_group + treatment * age_group, data = df) summary(naive_model) ## ## Call: ## lm(formula = vo2_max ~ treatment + age_group, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7165 -0.9743 -0.0066 0.9249 3.8438 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.17826 0.04955 689.75 &lt;2e-16 *** ## treatment 5.76923 0.05850 98.62 &lt;2e-16 *** ## age_group 2.18968 0.05850 37.43 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.306 on 1997 degrees of freedom ## Multiple R-squared: 0.8527, Adjusted R-squared: 0.8526 ## F-statistic: 5780 on 2 and 1997 DF, p-value: &lt; 2.2e-16 summary(adjusted_model) ## ## Call: ## lm(formula = vo2_max ~ treatment + age_group + treatment * age_group, ## data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0857 -0.6261 -0.0042 0.6543 3.3876 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.98227 0.04273 818.756 &lt; 2e-16 *** ## treatment 4.05124 0.06246 64.866 &lt; 2e-16 *** ## age_group 0.49082 0.06211 7.903 4.47e-15 *** ## treatment:age_group 3.43225 0.08828 38.880 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9855 on 1996 degrees of freedom ## Multiple R-squared: 0.9162, Adjusted R-squared: 0.9161 ## F-statistic: 7272 on 3 and 1996 DF, p-value: &lt; 2.2e-16 If we look only at the naive model without considering effect modification, then we would have incorrectly concluded that the drug impacts young and old people in the same way, adding 5.76 mL/kg/min to a patient’s VO2 max on average. If we look at the adjusted model considering effect modification, then we see that we more accurately estimate the ground truths. We also have the correct interpretation that the drug’s effect varies depending on whether the patient is young or old. For old people, the drug is estimated to increase the VO2 max by 4.05. For young people, the drug is estimated to increase the VO2 max by 4.05 + 3.43 = 7.48. So the drug is 3.43 units more effective in young people. The following plot shows how younger patients benefit more from the drug than older patients. df_grouped &lt;- df |&gt; mutate(age_group = ifelse(age_group == 1, &quot;Young&quot;, &quot;Old&quot;)) ggplot(df_grouped, aes(x = treatment, y = vo2_max, color = age_group)) + geom_jitter(width = 0.1, alpha = 0.4) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 3, shape = 18) + stat_summary(fun = mean, geom = &quot;line&quot;, aes(group = age_group)) + scale_x_continuous(breaks = c(0, 1)) + labs(title = &quot;Effect Modification: Drug and Age Category&quot;, x = &quot;Treatment (0 = Control, 1 = Drug)&quot;, y = &quot;VO2 Max (mL/kg/min)&quot;, color = &quot;Age Category&quot;) + theme_minimal() 4.2 Interaction When we study interactions, we are trying to understand the joint effect of two or more treatments on a certain outcome. So we answer the causal question “What is the combined effect of treatment 1 and treatment 2 on an outcome?”. Simulation to Demonstrate Interaction Let’s say we are interested in understanding the effect of new drug and taking vitamin supplements on a person’s VO2 max. Assume for this example that the patients who take vitamin supplements take the same supplements in the same dosage. In this simulation, the drug increases VO2 max by 4 mL/kg/min, vitamins increase VO2 max by 1.5 mL/kg/min, and taking both the drug and vitamins adds an addition 2.5 mL/kg/min to the VO2 max. set.seed(123) n &lt;- 2000 A_1 &lt;- rbinom(n, 1, 0.5) # drug A_2 &lt;- rbinom(n, 1, 0.5) # vitamin supplementation baseline &lt;- 35 # baseline VO2 max is 35 mL/kg/min vo2_max &lt;- baseline + 4 * A_1 + 1.5 * A_2 + 2.5 * (A_1 * A_2) + rnorm(n, mean = 0, sd = 1) df &lt;- data.frame(vo2_max, A_1, A_2) naive_model &lt;- lm(vo2_max ~ A_1 + A_2, data = df) adjusted_model &lt;- lm(vo2_max ~ A_1 + A_2 + A_1 * A_2, data = df) summary(naive_model) ## ## Call: ## lm(formula = vo2_max ~ A_1 + A_2, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4558 -0.8316 0.0082 0.7735 3.6096 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.41251 0.04392 783.61 &lt;2e-16 *** ## A_1 5.26868 0.05184 101.62 &lt;2e-16 *** ## A_2 2.69471 0.05184 51.98 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.158 on 1997 degrees of freedom ## Multiple R-squared: 0.8723, Adjusted R-squared: 0.8722 ## F-statistic: 6820 on 2 and 1997 DF, p-value: &lt; 2.2e-16 summary(adjusted_model) ## ## Call: ## lm(formula = vo2_max ~ A_1 + A_2 + A_1 * A_2, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0857 -0.6261 -0.0042 0.6543 3.3876 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.98227 0.04273 818.76 &lt;2e-16 *** ## A_1 4.05124 0.06246 64.87 &lt;2e-16 *** ## A_2 1.49082 0.06211 24.00 &lt;2e-16 *** ## A_1:A_2 2.43225 0.08828 27.55 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9855 on 1996 degrees of freedom ## Multiple R-squared: 0.9075, Adjusted R-squared: 0.9073 ## F-statistic: 6526 on 3 and 1996 DF, p-value: &lt; 2.2e-16 If we look at the naive model, we would conclude that the drug increase VO2 max by 5.26 and the vitamins increase VO2 max by 2.69. But this isn’t the truth. The naive model doesn’t capture the combined effect of taking both the drug and vitamins, which is capture in the adjusted model using an interaction term. Based on the adjusted model, we would conclude that the drug increases VO2 max by 4.05, the vitamins increase VO2 max by 1.49, and taking both adds 2.43 mL/kg/min to one’s VO2 max on average. This is closer to the ground truth that we established when explaining the simulation. This plot shows that taking the drug increases VO2 max, but also taking vitamins with the drug has an combined effect that increases VO2 max. # library(ggplot2) # library(dplyr) df_grouped &lt;- df |&gt; mutate(vitamin_group = ifelse(A_2 == 1, &quot;Vitamins&quot;, &quot;No Vitamins&quot;)) ggplot(df_grouped, aes(x = A_1, y = vo2_max, color = vitamin_group)) + geom_jitter(width = 0.1, alpha = 0.4) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 3, shape = 18) + stat_summary(fun = mean, geom = &quot;line&quot;, aes(group = vitamin_group)) + scale_x_continuous(breaks = c(0, 1)) + labs(title = &quot;Interaction: Drug × Vitamin Supplement&quot;, x = &quot;Treatment (0 = Control, 1 = Drug)&quot;, y = &quot;VO2 Max (mL/kg/min)&quot;, color = &quot;Vitamin Group&quot;) + theme_minimal() In this simulation, we learned that when we are interested in measuring the effect of multiple treatments on an outcome, it’s important to consider the combined effect of these treatments. We achieve this by adding an interaction term in our regression model. 4.3 Identifying Interaction Identifying interaction is crucial when studying causal relationships because it tells us whether there is a combined effect of two or more treatments on an outcome. Rather than being a source of bias like confounding, interaction reveals real differences in how subgroups respond to exposures or treatments. Detecting interaction helps researchers understand for whom and under what conditions an intervention works best, allowing for more tailored public health strategies and clinical recommendations. In our simulation with the drug and vitamin supplements, interaction was present because there was a combined effect of the drug and vitamin supplementation on VO2 max. In particular, patients that took both had the largest increases in VO2 max. By fitting a model with an interaction term between the drug and vitamin use, we were able to identify and quantify this combined interaction effect. Without testing for interaction, we would have incorrectly assumed that the drug and the vitamins independently improve VO2 max. 4.4 Effect Modification vs Interaction Effect modification and interaction are closely related concepts, but they serve slightly different purposes in causal analysis. The confusing part is when we try to model both effect modification and interaction, we do it the same way with an interaction term in the regression. However, the difference between the two concepts is in the causal questions we are trying to answer. In effect modification, we are trying to understand how the effect of a single treatment varies based on levels of another factor like age or sex assigned at birth. In interaction, we are trying to understand the combined effect of two or more treatments such as a drug and vitamin supplementation. "],["introduction-to-causal-diagrams.html", "5 Introduction to Causal Diagrams Class materials Textbook reading Supplementary reading Topics covered 5.1 Basic principles of directed acyclic graphs (DAGs) 5.2 Common Causal Structures in Public Health 5.3 Application: drawing DAGs for public health scenarios", " 5 Introduction to Causal Diagrams Class materials Slides: Module 5 Recording: Module 5, Part 1 Recording: Module 5, Part 2 Textbook reading Hernán &amp; Robins, Causal Inference: What If – Chapters 6-7 Supplementary reading Rosenbaum, P.R., &amp; Rubin, D.B. (1983). The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1), 41–55. Examples from public health studies involving confounding adjustment Topics covered Basic principles of directed acyclic graphs (DAGs) Common causal structures in public health Application: drawing DAGs for public health scenarios 5.1 Basic principles of directed acyclic graphs (DAGs) Directed acyclic graphs (DAGs) are powerful tools in causal inference that visually represent assumptions about how variables are related. In a DAG, nodes represent variables, and arrows (directed edges) represent causal influences from one variable to another. DAGs are acyclic, meaning you cannot return to the same variable by following a sequence of arrows — this prevents feedback loops. These graphs also are directed, meaning that they show the ways we think the causal relationships flow from one variable to another using the arrows. The key strength of DAGs lies in their ability to clarify causal pathways, distinguish between confounding and mediation, and identify the variables we need to control for to estimate causal effects accurately. By encoding assumptions explicitly, DAGs help researchers determine whether observed associations reflect true causal relationships or are biased by omitted variables or incorrect conditioning. In our simulation, we use a DAG to represent a common public health structure involving diet, exercise, and heart health. Diet is a confounder: it directly influences both how much people exercise and their overall heart health. If we ignore diet when estimating the effect of exercise on heart health, we risk attributing diet’s effect to exercise — leading to confounding bias. The DAG for this scenario includes arrows from diet to both exercise and heart health, and from exercise to heart health. # library(ggplot2) # library(dplyr) # library(ggdag) # library(dagitty) # library(bnlearn) # library(igraph) set.seed(123) n &lt;- 2000 diet &lt;- rnorm(n) exercise &lt;- 2 * diet + rnorm(n) heart_health &lt;- 3 * exercise + 4 * diet + rnorm(n) df &lt;- data.frame(diet, exercise, heart_health) dag &lt;- model2network(&quot;[diet][exercise|diet][heart_health|diet:exercise]&quot;) g &lt;- bnlearn::as.igraph(dag) plot(g, layout = layout_as_tree(g, root = &quot;diet&quot;), vertex.label.color=&quot;black&quot;, vertex.size=60, edge.arrow.size=0.5) 5.2 Common Causal Structures in Public Health In public health research, understanding the causal relationships between variables is essential for identifying risk factors, designing interventions, and making policy decisions. Common causal structures include confounding, mediation, and colliders, each of which influences how we interpret observed associations. A confounder is a variable that affects both the exposure and the outcome, potentially biasing the estimated effect if not properly adjusted for. Assume we had a hypothesis that ice cream sales cause murder rates. We would draw the DAG below, and show that outside temperature is a confounder, since it affects both ice cream sales (the exposure) and murder rate (the outcome). A mediator lies on the causal pathway between exposure and outcome, helping to explain how the exposure exerts its effect. In the example below, weight loss is the mediator because the exposure exercise causes weight loss, which affects the risk of diabetes. Based on this DAG, we are assuming that the only way exercise affects diabetes risk is through weight loss. In reality, this might not be true, but we kept the structure of this DAG simple for example’s sake. A collider, on the other hand, is influenced by two variables, and conditioning on it can introduce spurious associations. In the example below, the lawn being wet is a collider because it is caused by both by whether the sprinkler turns on and whether it rains outside. An easy way to remember if a variable is a collider is if it is a variable where two arrows point toward (or collide into) the variable. Identifying these structures often requires drawing directed acyclic graphs (DAGs) to map out assumptions and determine which variables to adjust for when estimating causal effects. They also help us understand the flow of associations between the different variables. We use paths to track the flow of association. A backdoor path is any path from the exposure to the outcome with an arrow pointing towards the exposure. For example, in the ice cream and murder rate DAG, there is a backdoor path Ice Cream Sales \\(\\rightarrow\\) Outside Temperature \\(\\leftarrow\\) Murder Rate since the arrow between Outside Temperature and Murder Rate points back at Ice Cream Sales. Paths can either be closed or open. Opening a path allows association to flow from one variable to another, while closing does not. In general, we want to close all backdoor paths keep all other paths open by conditioning on set of specific variables. This will guarantee conditional exchangeability, allowing us to identify the causal effect. In our first simulation, we modeled a classic confounding structure, where diet influences both exercise and heart health. This mirrors real-world public health situations where health behaviors and biological outcomes are shaped by shared underlying factors like nutrition, socioeconomic status, or genetics. If we were to estimate the effect of exercise on heart health without adjusting for diet, we would risk attributing some of diet’s impact to exercise — a classic confounding problem. By visualizing the relationships using a DAG and including diet as a covariate in our regression model, we can block the backdoor path and isolate the true causal effect of exercise. This illustrates how understanding and modeling common causal structures is critical to producing valid and meaningful results in public health research. 5.3 Application: drawing DAGs for public health scenarios When modeling a relationship such as the effect of a doctor’s visit on sickness level, there are so many factors that can make a causal diagram extremely complex. In this example, there can be so many confounders such as whether the person has insurance, whether the patient was sick before, whether the person can afford to live in a clean area, whether they can take time off work to go to the doctor’s office, and so on. There appear to be too many factors that we would have to control for to identify the causal effect, but we can follow some steps to simplify and create our causal diagram. We apply steps to the examples below: List out all factors - Going to the doctor’s office, insurance status, sickness level before treatment, cleanliness of living area, available time to take off work to go to doctor’s office. Prune and combine factors based on domain knowledge - Using domain knowledge, some of the factors are related in similar ways. For example, a person’s insurance status, the quality of their living area, and whether or not they have time to take off work and go to the doctor’s all affect whether they go to the doctor’s office and their sickness level after going to the doctor’s in similar ways. Namely, the higher quality the insurance, living area, and amount of time off available, the more likely the person is going to go to a doctor’s office and be in better health on average. We can simplify these three factors into a single one such as economic status. After simplification, we now have four variables: doctor’s office visit, economic status, sickness status before visit, sickness level after visit. Add causal relationships - After simplifying our factors, we then start drawing arrows that represent the direction that causation flows from one variable to another. For example, we can hypothesize that visiting a doctor will affect a person’s sickness level, and economic status and pre-sickness level are confounders. The direction of these arrows is based on our assumptions and domain knowledge. "],["confounding-and-selection-bias.html", "6 Confounding and Selection Bias Class materials Textbook reading Supplementary reading Topics Covered 6.1 The Form of Selection Bias 6.2 How to Adjust for Selection Bias 6.3 The Structure of Confounding 6.4 How to Adjust for Confounding", " 6 Confounding and Selection Bias Class materials Slides: Module 6 Recording: Module 6, Part 1 Recording: Module 6, Part 2 Textbook reading Hernán &amp; Robins, Causal Inference: What If – Chapter 7-8 Supplementary reading Knol, M. J., &amp; VanderWeele, T. J. (2012). Recommendations for presenting analyses of effect modification and interaction. International Journal of Epidemiology, 41(2), 514–520. Real-world public health examples of effect modification Topics Covered The form of selection bias How to adjust for selection bias The structure of confounding How to adjust for confounding Critical reading exercise: sources of confounding and selection bias in public health 6.1 The Form of Selection Bias Selection bias occurs when the probability of being included in the study or analysis depends on a factor that affects the association between the treatment and the outcome. Selection bias occurs when we condition (by stratifying or controlling) on a collider or mediator. When we unintentionally condition on these causal structures, we make our estimates biased. Conditioning on a collider opens up a backdoor path, biasing our estimate. Conditioning on a mediator blocks a path that captures the effect an exposure can have on an outcome through the mediator, also biasing our estimate. In our simulation, we are interested in studying the effect of having a high income on whether a person gets regular exercise or not. We also have data on healthcare visits for each patient, which is affected both by whether they have high income and whether they exercise. For this example, assume that we do not know the true effect of income on exercise, but in reality, there is no causal effect. Since we believe there is an effect, we will show this causal relationship in our DAG. The DAG for this scenario is shown below. We introduce selection bias by adjusting on healthcare visits (a collider) in our regression. By conditioning on healthcare visits, our estimated effect will contain the association from the backdoor path. The code below estimates the effect of heart health on exercise using two models. The first model does not control for healthcare visits and unbiasedly estimates the true causal effect of 0. The second model controls for healthcare visits and suffers from selection bias. set.seed(123) n &lt;- 20000 # Income and exercise are independent (no true causal effect for this example) high_income &lt;- rbinom(n, 1, 0.4) # 40% have high income regular_exercise &lt;- rbinom(n, 1, 0.3) # 30% exercise regularly # Healthcare_visits is a collider of high_income and regular_exercise healthcare_visits &lt;- 2 + 1.5 * high_income + 1.2 * regular_exercise + rnorm(n, 0, 0.8) df &lt;- data.frame( high_income = high_income, regular_exercise = regular_exercise, healthcare_visits = healthcare_visits ) # Model 1 — No adjustment (baseline) model_unadjusted &lt;- lm(regular_exercise ~ high_income, data = df) # Model 2 — Adjusting for the collider (bad) model_adjusted_collider &lt;- lm(regular_exercise ~ high_income + healthcare_visits, data = df) # Extract coefficients coef_unadjusted &lt;- summary(model_unadjusted)$coefficients[&quot;high_income&quot;, &quot;Estimate&quot;] coef_adjusted &lt;- summary(model_adjusted_collider)$coefficients[&quot;high_income&quot;, &quot;Estimate&quot;] # Compare results cat(&quot;Coefficient without adjustment (true effect):&quot;, coef_unadjusted, &quot;\\n&quot;) ## Coefficient without adjustment (true effect): 0.004749401 cat(&quot;Coefficient with collider adjustment (biased):&quot;, coef_adjusted, &quot;\\n&quot;) ## Coefficient with collider adjustment (biased): -0.3942755 The previous scenario where we condition on a collider is more specifically known as collider bias. Had we conditioned on a mediator, this would have been mediator bias. In the example about the effect of exercise on diabetes risk, if we were to condition on weight loss when estimating our causal effect, we would block the causal path of exercise \\(\\rightarrow\\) weight loss \\(\\rightarrow\\) diabetes risk and not be able to estimate the effect of exercise on diabetes risk, mediated by weight loss. 6.2 How to Adjust for Selection Bias Adjusting for selection bias is more challenging than adjusting for confounding, because selection bias arises when the sample being analyzed is not representative of the target population due to a systematic inclusion process. This often happens when selection into the dataset depends on variables related to both the exposure and the outcome, introducing a spurious association that distorts causal estimates. Unlike confounding, which can often be handled by conditioning on measured variables, selection bias may require more complex strategies such as inverse probability weighting (IPW), sensitivity analysis, or explicitly modeling the selection mechanism. The key to adjusting for selection bias is understanding why and how certain individuals are excluded or included in the analysis — and then incorporating that information to correct the bias. 6.3 The Structure of Confounding Confounding occurs when an external variable influences both the exposure and the outcome, making it difficult to determine whether the observed association is truly causal. This third variable — the confounder — can create a misleading impression that the exposure causes the outcome, when in fact, the association may be driven entirely or partially by the confounder. The key structural feature of confounding is that the confounder must be related to both the exposure and the outcome. To obtain an accurate estimate of the exposure’s causal effect, researchers must adjust for confounding variables using methods like stratification, regression, or matching. In our simulation, age acts as a confounder because it affects both smoking behavior and lung cancer risk. Older individuals are more likely to smoke and also more likely to develop lung cancer, which can make smoking appear more harmful (or even less harmful) than it actually is if age isn’t taken into account. The naive model, which includes only smoking, produces a biased estimate because it doesn’t separate the effect of smoking from the effect of age. The adjusted model includes both smoking and age and provides a more accurate estimate of smoking’s effect by accounting for this confounding influence. This example highlights how confounding can distort findings and why controlling for related background variables is essential in observational research. For this simulation, assume smoking has a true effect of 3 on lung cancer. The DAG for this scenario is below: set.seed(123) n &lt;- 2000 age &lt;- rnorm(2000, mean = 50, sd = 10) smoking &lt;- 2 * age + rnorm(n) lung_cancer &lt;- 3 * smoking + 4 * age + rnorm(n) genetic_marker &lt;- rbinom(n, 1, prob = plogis(0.01 * smoking - 1)) df &lt;- data.frame( Age = age, Smoking = smoking, Lung_Cancer = lung_cancer, Genetic_Marker = genetic_marker ) model_naive &lt;- lm(lung_cancer ~ smoking, data = df) model_adjusted &lt;- lm(lung_cancer ~ smoking + age, data = df) coef_naive &lt;- summary(model_naive)$coefficients[&quot;smoking&quot;, &quot;Estimate&quot;] coef_adjusted &lt;- summary(model_adjusted)$coefficients[&quot;smoking&quot;, &quot;Estimate&quot;] coef_naive ## [1] 4.995915 coef_adjusted ## [1] 3.014671 Through this simulation, we learned that controlling for confounders via adjusting for them in the regression model helps prevent our estimates from being biased. Practically, in the context of public health, there will always be unobserved confounders that we can’t control for. This makes it impossible to control for all confounders. This does not mean that it is impossible to have credible causal estimates. Instead of trying to get an unbiased estimate by controlling for all confounders (which is infeasible), researchers acknowledge that unobserved confounders exist and can affect their estimates. Through a process called sensitivity analysis researchers can observe how strong a confounder has to be in order to invalidate their results. In the example above, our estimated effect after properly controlling for age is around 3. However, it is possible that we forgot to account for a genetic marker that is a confounder of smoking and lung cancer. In practice, it is infeasible to control for the genetic marker of each individual in our study, so we use sensitivity analysis to see how robust our estimates are. In the code below, we simulate how strong the association between genetic marker on smoking and the association between genetic marker and lung cancer has to be in order to invalidate our results. The heatmap shows the effect estimates for varying strengths of confounding on the exposure and outcome. Our new DAG including the genetic marker would look like the following: set.seed(123) n &lt;- 1000 age &lt;- rnorm(n) results &lt;- data.frame() a_vals &lt;- seq(0, 10, by = 1) # strength of genetic_marker on smoking b_vals &lt;- seq(0, -20, by = -1) # strength of genetic_marker on lung_cancer for (a in a_vals) { for (b in b_vals) { genetic_marker &lt;- rbinom(n, 1, 0.5) smoking_resid &lt;- a * genetic_marker + rnorm(n) smoking_resid &lt;- as.numeric(scale(smoking_resid)) # Fix variance to 1 to avoid inflating variance of smoking smoking &lt;- 4 * age + smoking_resid lung_cancer &lt;- 3 * smoking + 4 * age + b * genetic_marker + rnorm(n) df &lt;- data.frame(lung_cancer, smoking, age) model &lt;- lm(lung_cancer ~ smoking + age, data = df) est &lt;- coef(summary(model))[&quot;smoking&quot;, &quot;Estimate&quot;] se &lt;- coef(summary(model))[&quot;smoking&quot;, &quot;Std. Error&quot;] results &lt;- rbind(results, data.frame(a = a, b = b, smoking_estimate = est, se = se)) } } ggplot(results, aes(x = a, y = b, fill = smoking_estimate)) + geom_tile() + scale_fill_gradient( low = &quot;white&quot;, high = &quot;Blue&quot;, breaks = seq(3, -6, by = -1.5) ) + labs( x = &quot;Strength of Confounder on Smoking&quot;, y = &quot;Strength of Confounder on Lung Cancer&quot;, fill = &quot;Smoking Effect&quot;, title = &quot;Effect of Smoking Estimate by Confounder Strength&quot; ) + theme_minimal() + scale_y_reverse() When identifying a causal effect by assuming conditional exchangeability, sensitivity analysis is incredibly important to defending claims of causality. For example, based off this graph, one could argue that the strength of the genetic marker is likely not strong enough to change our result that smoking increases the chance of lung cancer. 6.4 How to Adjust for Confounding Adjusting for confounding is essential when estimating causal effects from observational data. Since confounders are variables that influence both the exposure and the outcome, failing to account for them can lead to biased and misleading conclusions. One of the most common ways to adjust for confounders is through multiple regression, where confounders are included as covariates in the model. Other methods include stratification, where analyses are performed within levels of the confounder, and matching, where exposed and unexposed individuals are paired based on similar values of the confounding variable. These approaches aim to isolate the effect of the exposure by holding confounders constant, thereby mimicking the balance achieved in randomized experiments. In our simulation, we demonstrated adjustment for confounding using regression. The variable age was a confounder because it influenced both smoking and lung cancer. When we fit a naive model that only included smoking, the effect estimate was biased because it reflected both smoking’s and age’s contributions to lung cancer. By including age in the model as an additional predictor, we were able to adjust for its influence. This adjustment allowed us to estimate the effect of smoking on lung cancer more accurately, as if age were held constant. This simple regression approach illustrates a key principle in observational research: if you can measure the confounder and include it in your analysis, you can often remove its biasing effect and get closer to the true causal relationship. "],["other-common-pitfalls-of-causal-analyses.html", "7 Other Common Pitfalls of Causal Analyses Class materials Textbook reading Supplementary reading Topics covered 7.1 Measurement Bias 7.2 Non-compliance 7.3 Non-Causal Diagrams 7.4 Publication Bias and P-Hacking 7.5 Over- and Mis-Interpretation of Statistical Analyses", " 7 Other Common Pitfalls of Causal Analyses Class materials Slides: Module 7 Recording: Module 7, Part 1 Recording: Module 7, Part 2 Textbook reading Hernán &amp; Robins, Causal Inference: What If – Chapter 9 Supplementary reading ADD RELEVANT READING Examples of misclassification and selection bias in public health research Topics covered Measurement bias Non-compliance Non-causal diagrams Publication bias and p-hacking Over- and mis-interpretation of statistical analyses Application: developing a checklist for critical reading of causal claims 7.1 Measurement Bias Measurement bias occurs when the method used to collect data leads to systematic errors in the values recorded for a variable. This can happen when an exposure, outcome, or confounder is misclassified or inaccurately measured in a way that consistently overstates or understates the true value. Measurement bias is problematic because it can distort observed associations and lead to incorrect conclusions about the relationships between variables. Unlike random measurement error, which tends to cancel out over large samples, measurement bias introduces consistent errors that don’t disappear with more data. It can arise from faulty instruments, poorly worded survey questions, or inconsistent data collection procedures, and it often goes unnoticed unless explicitly tested for. In public health and medical research, measurement bias can affect both exposure and outcome variables. For example, if smoking behavior is self-reported and individuals tend to underreport how much they smoke, the study will underestimate the true relationship between smoking and lung cancer. Similarly, if age is recorded in broad categories rather than precise years, it can limit the ability to adjust accurately for confounding. Even adjusting for confounders may not correct measurement bias if those confounders are also measured with error. This makes it critical to use reliable, validated measurement tools and to account for potential misclassification during analysis, especially in observational studies where data quality may vary widely. This simulation demonstrates measurement bias by comparing the estimated effect of smoking on lung cancer using the true smoking values versus mismeasured (underreported) smoking. The model using mismeasured smoking underestimates the true effect, showing how systematic error in recording an exposure can bias causal estimates toward zero. n &lt;- 2000 age &lt;- rnorm(n, mean = 50, sd = 10) true_smoking &lt;- 2 * age + rnorm(n) # no age to isolate measurement bias in smoking lung_cancer &lt;- 3 * true_smoking + rnorm(n) measured_smoking &lt;- true_smoking - rnorm(n, mean = 1, sd = 0.5) true_model &lt;- lm(lung_cancer ~ true_smoking + age) biased_model &lt;- lm(lung_cancer ~ measured_smoking + age) true_coef &lt;- summary(true_model)$coefficients[&quot;true_smoking&quot;, &quot;Estimate&quot;] biased_coef &lt;- summary(biased_model)$coefficients[&quot;measured_smoking&quot;, &quot;Estimate&quot;] true_coef ## [1] 3.008776 biased_coef ## [1] 2.445573 We can capture measurement bias in DAGs as well by denoting the measured exposure variable as \\(W^*\\). In the example below, the measured treatment \\(W^*\\) is affected both by confounders and the exposure variable \\(W\\). 7.2 Non-compliance So far we have assumed that the examples with randomized experiments that we have covered went as intended. In real experiments, sometimes participants might not comply with the treatment they are assigned to. For example, if our example is about the effect of a heart transplant on 5-year mortality, some patients might not undergo surgery even if they were assigned the surgery. To account for the potential mismatch between the treatment assigned and the treatment received for a patient in DAG notation, we use two variables. \\(Z\\) represents the assigned treatment, and \\(W\\) represents the received treatment. This is different from how we denote measurement bias because \\(Z\\) can have a causal effect on \\(Y\\). 7.3 Non-Causal Diagrams Non-causal diagrams represent associations between variables that do not imply direct cause-and-effect relationships. These diagrams are useful for illustrating statistical relationships that arise from shared causes, correlations due to bias, or measurement artifacts. In non-causal diagrams, arrows may still indicate directional influence, but they are used to reflect associations or data-generating processes, not claims about interventions. Unlike causal diagrams, which are designed to identify and estimate the effects of manipulating one variable on another, non-causal diagrams help clarify patterns in the data without asserting that changing one variable will necessarily change another. In the context of our simulation, we can use a non-causal diagram to represent the observed association between age and lung cancer without assuming a direct causal relationship. While age and lung cancer may be strongly correlated — older individuals tend to have higher cancer risk — this relationship does not imply that age causes lung cancer in an interventional sense. Instead, age may be acting as a proxy for other underlying factors like cumulative exposure to smoking or environmental risks. A non-causal diagram helps us visualize this statistical association without attributing it to a direct, manipulable pathway, highlighting that not all observed relationships in data should be interpreted as causal. 7.4 Publication Bias and P-Hacking Publication bias occurs when the likelihood of a study being published depends on the nature or direction of its results — typically favoring studies with statistically significant or “positive” findings. This creates a distorted picture of the evidence in a field, because null or contradictory results are less likely to be seen. P-hacking refers to the practice of manipulating statistical analyses or data collection until a desired (usually statistically significant) result is achieved. This can include selectively reporting outcomes, running many analyses and only publishing those with low p-values, or stopping data collection once a significant result appears. Both practices inflate false-positive rates and undermine the credibility of scientific findings. In the context of the simulation we ran — whether it involves confounding, selection bias, or measurement error — it’s easy to see how p-hacking or publication bias could skew interpretations. For example, imagine rerunning the simulation many times and only reporting the version where the naive model shows a significant effect of smoking on lung cancer (even if the underlying data or causal structure doesn’t support it). Or selectively reporting only the adjusted model that produces a “clean” result while hiding others. These practices can make even a carefully designed simulation appear misleading. The simulation reinforces the idea that statistical significance is not the same as truth, and that transparency in modeling choices and full reporting of results are critical for avoiding biased conclusions. 7.5 Over- and Mis-Interpretation of Statistical Analyses Over-interpretation occurs when researchers draw stronger conclusions from statistical results than the data can justify, while mis-interpretation involves misunderstanding what the results actually mean. A common example is interpreting a statistically significant association as proof of causation, even when the study design or model does not support that claim. Another frequent error is overstating the practical importance of a small effect size or assuming that a non-significant result means “no effect.” These issues are often driven by pressure to produce definitive conclusions, even when the data are limited, noisy, or confounded. Careful interpretation requires understanding the limits of the methods used and being transparent about uncertainty, assumptions, and alternative explanations. In the simulations we’ve conducted — such as estimating the effect of smoking on lung cancer under different types of bias — it’s easy to see how results can be over- or mis-interpreted. For instance, in a model affected by measurement bias or confounding, one might find a statistically significant association between smoking and lung cancer, but incorrectly conclude that the estimated effect size reflects the true causal effect. Alternatively, if the biased model appears significant and the true model does not, someone might misinterpret that as evidence that adjustment “eliminated” the effect, when in fact it corrected for bias. These examples highlight how even simple models can be misunderstood or overstated, and underscore the importance of grounding interpretation in study design, data limitations, and causal reasoning — not just statistical output. "],["from-identification-to-estimation.html", "8 From Identification to Estimation Class materials Textbook reading Supplementary reading Topics covered 8.1 Identification and Estimation 8.2 Estimation of causal effects 8.3 Taxonomy of estimation models", " 8 From Identification to Estimation Class materials Slides: Module 8 Recording: Module 8, Part 1 Recording: Module 8, Part 2 Textbook reading Hernán &amp; Robins, Causal Inference: What If – Chapter 10 Supplementary reading ADD RELEVANT READING Case studies on unmeasured confounding and robustness of findings Topics covered Identification versus estimation Estimation of causal effects Taxonomy of estimation models Critical reading exercise: evaluating identification and estimation in a published study 8.1 Identification and Estimation In causal inference, identification refers to the theoretical question of whether a causal effect can be determined from the observed data and the assumptions encoded in the study design or model. It answers the question: “Can we, in principle, express the causal effect of interest as a function of the observed variables?” Identification depends on assumptions such as exchangeability (no unmeasured confounding), positivity, and consistency, and often involves tools like potential outcomes or directed acyclic graphs (DAGs). If a causal effect is not identifiable, no amount of statistical analysis will yield a valid estimate — because the data alone cannot disentangle the causal effect from bias or confounding. Once identification is established, the next step is estimation, which involves applying statistical methods to calculate the size of the effect using real data. Estimation uses techniques like regression, inverse probability weighting, or matching to quantify the identified causal relationship. In our simulations, for example, once we specify that the causal effect of smoking on lung cancer is identifiable by adjusting for age, we use regression to estimate that effect. If we skip the identification step and go straight to estimation without accounting for confounding or bias, our estimates may be precise — but wrong. Together, identification and estimation form the backbone of credible causal analysis: one ensures that we’re asking the right question, and the other that we’re answering it correctly. ADD AN EXAMPLE OF GOING END-TO-END FROM CAUSAL QUESTION -&gt; IDENTIFICATION STRATEGY -&gt; DEFENDING ASSUMPTIONS –&gt; ESTIMATION 8.2 Estimation of causal effects Estimation of causal effects refers to the process of using statistical methods to quantify the size and direction of a causal relationship between an exposure and an outcome. Once a causal effect has been identified—meaning that, under certain assumptions, it can be expressed in terms of observed variables—estimation allows us to compute a numerical value for that effect using data. Common estimation techniques include linear regression, inverse probability weighting, and matching. The goal is not just to observe an association, but to measure how much changing the exposure would change the outcome, assuming the identification conditions are satisfied. 8.3 Taxonomy of estimation models The taxonomy of estimation models refers to the classification of different statistical approaches used to estimate causal effects based on the structure of the data and the assumptions made. Broadly, estimation models fall into categories such as outcome regression (e.g., linear or logistic regression), exposure modeling (e.g., propensity scores), and doubly robust methods that combine both. These models vary in how they handle confounding, missing data, and complexity of relationships between variables. Choosing the appropriate estimation model depends on the research question, the nature of the confounding, and how well the model’s assumptions align with the underlying causal structure. "],["applications-of-causal-inference-in-public-health-and-medical-research.html", "9 Applications of Causal Inference in Public Health and Medical Research Class materials Textbook reading Supplementary reading Topics covered 9.1 Public health policy evaluation 9.2 Environmental exposure studies 9.3 Health disparities research 9.4 Prevention program evaluation", " 9 Applications of Causal Inference in Public Health and Medical Research Class materials Slides: Module 9 Recording: Module 9, Part 1 Recording: Module 9, Part 2 Textbook reading Hernán &amp; Robins, Causal Inference: What If – Chapter 15 Supplementary reading Sterman, J. D. (2006). Learning from evidence in a complex world. American Journal of Public Health, 96(3), 505–514. Examples of systems-level interventions in public health and policy Topics covered Public health policy evaluation Environmental exposure studies Health disparities research Prevention program evaluation 9.1 Public health policy evaluation n &lt;- 2000 baseline_health &lt;- rnorm(n, mean = 50, sd = 10) policy &lt;- ifelse(baseline_health + rnorm(n, 0, 5) &lt; 52, 1, 0) health_outcome &lt;- 5 * policy + 0.6 * baseline_health + rnorm(n, 0, 5) df &lt;- data.frame( Policy = factor(policy, labels = c(&quot;No Policy&quot;, &quot;Policy&quot;)), Baseline_Health = baseline_health, Health_Outcome = health_outcome ) model_naive &lt;- lm(Health_Outcome ~ Policy, data = df) model_adjusted &lt;- lm(Health_Outcome ~ Policy + Baseline_Health, data = df) summary(model_naive)$coefficients[&quot;PolicyPolicy&quot;, ] ## Estimate Std. Error t value Pr(&gt;|t|) ## -3.861950e+00 2.957099e-01 -1.305993e+01 1.821710e-37 summary(model_adjusted)$coefficients[&quot;PolicyPolicy&quot;, ] ## Estimate Std. Error t value Pr(&gt;|t|) ## 5.065309e+00 3.146141e-01 1.610007e+01 6.273699e-55 library(ggplot2) ggplot(df, aes(x = Policy, y = Health_Outcome, fill = Policy)) + geom_boxplot(alpha = 0.6) + labs(title = &quot;Simulated Impact of Public Health Policy&quot;, x = &quot;Policy Implemented&quot;, y = &quot;Health Outcome After 1 Year&quot;) + theme_minimal() 9.2 Environmental exposure studies Environmental exposure studies investigate how exposure to environmental factors—such as air pollution, contaminated water, radiation, or hazardous chemicals—affects human health. These studies aim to establish causal links between environmental conditions and outcomes like respiratory illness, cancer, or developmental disorders. Because randomizing exposure is often unethical or impractical, these studies typically rely on observational data, making careful adjustment for confounders and sources of bias essential. Tools like causal diagrams, regression models, and sensitivity analyses are often employed to assess whether the observed health effects are truly caused by the environmental exposure. In the policy simulation, the setup closely resembles an environmental exposure study where “policy” could be interpreted as an intervention to reduce environmental harm (e.g., enforcing clean air regulations). The simulation demonstrated how individuals exposed to the policy had lower average health outcomes, not because the policy failed, but because those individuals started with worse baseline health. This reflects a common challenge in environmental health research: exposure is often non-random and associated with other risk factors. Just like in environmental exposure studies, it is crucial to adjust for baseline differences—such as pre-existing health or socioeconomic status—to avoid incorrectly concluding that the exposure (or policy) caused harm. 9.3 Health disparities research Health disparities research focuses on understanding and addressing differences in health outcomes across population groups defined by factors such as race, ethnicity, socioeconomic status, geography, gender, or disability. These disparities often stem from unequal access to care, systemic discrimination, environmental exposures, and social determinants of health. The goal is not only to document these differences but also to identify causal pathways and implement interventions that promote health equity. Causal inference tools are especially valuable in this field because they help distinguish between mere correlations and actual structural inequalities that can be targeted through policy and intervention. The simulation of public health policy can be directly applied to health disparities research by modeling how an intervention affects different subgroups. For instance, if a health policy is implemented in lower-income neighborhoods, those individuals might begin with worse baseline health, as shown by lower health outcomes in the policy group. Without adjusting for these baseline differences, the simulation might misleadingly suggest that the policy worsens outcomes. In reality, this reflects the importance of stratifying or adjusting for social determinants when evaluating interventions aimed at reducing disparities. The simulation thus highlights how easily misleading conclusions can arise if disparities are not properly accounted for in causal analysis. 9.4 Prevention program evaluation Prevention program evaluation involves assessing the effectiveness of initiatives designed to reduce the risk of adverse health outcomes before they occur. These programs might target behaviors (e.g., smoking cessation), environmental risks (e.g., air quality improvement), or access to services (e.g., vaccination campaigns). Evaluators aim to determine whether the program caused measurable changes in outcomes such as disease incidence, risk factor reduction, or health equity. This requires careful consideration of confounding factors, selection bias, and whether the observed differences can be attributed to the intervention itself — making causal inference tools central to prevention research. In our simulation, the public health policy acts like a prevention program aimed at improving long-term health outcomes. By comparing the health trajectories of individuals in the policy versus no-policy groups, we can estimate the effect of the intervention. However, the simulation also reveals a key challenge: even if a program targets at-risk populations, initial disparities (like poorer baseline health) may mask its true benefits unless we adjust for those differences. This mirrors real-world prevention evaluations, where randomized trials or proper covariate adjustment are essential to distinguish actual program impact from background variability. "],["presentation-of-final-projects.html", "10 Presentation of Final Projects Class materials Guidelines 10.1 Final Project", " 10 Presentation of Final Projects Class materials Slides: Final Project Overview Recording: Project Presentation Session Guidelines Presentation of Final Projects Review of group projects 10.1 Final Project The final project is a group-based assignment designed to assess your ability to critically evaluate causal claims in real-world public health or epidemiological research. Working in teams of 3–4, you will select a published study and analyze it through the lens of causal inference. This includes clearly identifying the causal question the authors are attempting to answer, articulating the assumptions underlying their analysis, and evaluating whether the study’s design supports a valid causal interpretation. You’ll be expected to consider the use of tools like directed acyclic graphs (DAGs), potential confounders, selection bias, and whether the identification strategy is sound. In addition to identifying strengths and weaknesses in the study, your team will propose potential improvements or alternative approaches that could enhance causal validity. This might involve suggesting better adjustment strategies, different data collection designs, or more transparent modeling techniques. Ultimately, your goal is to interpret the study’s findings not just statistically, but causally — and explain their relevance for real-world public health policy or interventions. Your work will culminate in a 10-minute group presentation during Week 10 and a 3–4 page written report submitted during finals week, both of which demonstrate your ability to apply course concepts to actual scientific literature. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
